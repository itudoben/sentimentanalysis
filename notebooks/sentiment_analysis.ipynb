{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis of a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base path for the ACL IMDB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the reviews into data frames.\n",
    "base_path = \"/Users/hujol/Projects/advanced_analytics_spark/data/aclImdb/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# RegEx for extracting file information.\n",
    "prog = re.compile('(.*)_(.*)\\.txt')\n",
    "\n",
    "# Labels.\n",
    "labels = { 'pos': 1, 'neg':0}\n",
    "\n",
    "# Here for the sake of short testing without loading the whole data in Spark.\n",
    "ttl = 100000 # 50000\n",
    "ttl_h = ttl / 2\n",
    "\n",
    "# Try to get half of positives and negatives from ttl since my pc cannot load the total 55K reviews!\n",
    "ttl_positives = 0\n",
    "ttl_negatives = 0\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "# Features description.\n",
    "features = ['datasettype', 'filename', 'datetimecreated', 'reviewid', 'reviewpolarity', 'reviewrating', 'text']\n",
    "\n",
    "# The context.\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# The complete data frame built from all the files.\n",
    "df = None\n",
    "\n",
    "# Keep track of when the data set was built.\n",
    "utcdate = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "entries = []\n",
    "\n",
    "# Find all review files.\n",
    "for set_name in ('test', 'train'):\n",
    "    for sa_name in ('neg', 'pos'):\n",
    "        dir_path = os.path.join(base_path, set_name, sa_name)\n",
    "        \n",
    "        polarity = labels[sa_name]\n",
    "        \n",
    "        if ('neg' == sa_name and ttl_negatives > ttl_h): break;\n",
    "        \n",
    "        if ('pos' == sa_name and ttl_positives > ttl_h): break;\n",
    "        \n",
    "        # Load each review file.\n",
    "        for file in os.listdir(dir_path):\n",
    "            # Extract the ID and the rating of the review from the file name.\n",
    "            m = prog.match(file)\n",
    "            id = int(m.group(1))\n",
    "            rating = int(m.group(2))\n",
    "            \n",
    "            # Read in the review.\n",
    "            with open(os.path.join(dir_path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "                \n",
    "            # Prepare the entry\n",
    "            entry = [set_name, file, utcdate, id, int(polarity), int(rating), txt]\n",
    "            entries.append(entry)\n",
    "        \n",
    "            # Loop checking.\n",
    "            cnt += 1\n",
    "            if (cnt == ttl): break\n",
    "\n",
    "            if ('neg' == sa_name): ttl_negatives += 1\n",
    "            if ('neg' == sa_name and ttl_negatives > ttl_h): break;\n",
    "\n",
    "            if ('pos' == sa_name): ttl_positives += 1\n",
    "            if ('pos' == sa_name and ttl_positives > ttl_h): break;\n",
    "                \n",
    "        if (cnt == ttl): break\n",
    "    if (cnt == ttl): break\n",
    "\n",
    "# Process the entries.\n",
    "sa_rdd = sc.parallelize(entries)\n",
    "df = sa_rdd.toDF(features)\n",
    "        \n",
    "# Store the DF in parquet format.\n",
    "file_pqt = os.path.join(base_path, (\"aclImdb_%s_raw.parquet\" % ttl))\n",
    "\n",
    "# If the parquet directory exists, remove it.\n",
    "if Path(file_pqt).is_dir():\n",
    "    shutil.rmtree(file_pqt)\n",
    "\n",
    "df.write.parquet(file_pqt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "# Get the data as Pandas data frame.\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Re index to shuffle the data before saving it.\n",
    "pdf = pdf.reindex(np.random.permutation(pdf.index))\n",
    "pdf.to_csv(file_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV file for checking data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasettype</th>\n",
       "      <th>filename</th>\n",
       "      <th>datetimecreated</th>\n",
       "      <th>reviewid</th>\n",
       "      <th>reviewpolarity</th>\n",
       "      <th>reviewrating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>9487_1.txt</td>\n",
       "      <td>20181022T102331Z</td>\n",
       "      <td>9487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have seen this movie and I did not care for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>4604_4.txt</td>\n",
       "      <td>20181022T102331Z</td>\n",
       "      <td>4604</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>In Los Angeles, the alcoholic and lazy Hank Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datasettype    filename   datetimecreated  reviewid  reviewpolarity  \\\n",
       "1        test  9487_1.txt  20181022T102331Z      9487               0   \n",
       "2        test  4604_4.txt  20181022T102331Z      4604               0   \n",
       "\n",
       "   reviewrating                                               text  \n",
       "1             1  I have seen this movie and I did not care for ...  \n",
       "2             4  In Los Angeles, the alcoholic and lazy Hank Ch...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ttl = 100000\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "pdf_read = pd.read_csv(file_csv, encoding='utf-8')\n",
    "pdf_read[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "|datasettype|   filename| datetimecreated|reviewid|reviewpolarity|reviewrating|                text|\n",
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "|       test| 1821_4.txt|20181022T102331Z|    1821|             0|           4|Alan Rickman & Em...|\n",
      "|       test| 9487_1.txt|20181022T102331Z|    9487|             0|           1|I have seen this ...|\n",
      "|       test| 4604_4.txt|20181022T102331Z|    4604|             0|           4|In Los Angeles, t...|\n",
      "|       test| 2828_2.txt|20181022T102331Z|    2828|             0|           2|This film is bund...|\n",
      "|       test|10890_1.txt|20181022T102331Z|   10890|             0|           1|I only comment on...|\n",
      "|       test| 3351_4.txt|20181022T102331Z|    3351|             0|           4|When you look at ...|\n",
      "|       test| 8070_2.txt|20181022T102331Z|    8070|             0|           2|Rollerskating vam...|\n",
      "|       test| 1027_4.txt|20181022T102331Z|    1027|             0|           4|Technically abomi...|\n",
      "|       test| 8248_3.txt|20181022T102331Z|    8248|             0|           3|When Hollywood is...|\n",
      "|       test| 4290_4.txt|20181022T102331Z|    4290|             0|           4|Respected western...|\n",
      "|       test|10096_1.txt|20181022T102331Z|   10096|             0|           1|Worst movie ever ...|\n",
      "|       test|11890_1.txt|20181022T102331Z|   11890|             0|           1|I was forced to w...|\n",
      "|       test| 2008_1.txt|20181022T102331Z|    2008|             0|           1|Well it is about ...|\n",
      "|       test|  472_4.txt|20181022T102331Z|     472|             0|           4|Man with the Scre...|\n",
      "|       test| 9876_2.txt|20181022T102331Z|    9876|             0|           2|I never read the ...|\n",
      "|       test|11402_1.txt|20181022T102331Z|   11402|             0|           1|One of the movies...|\n",
      "|       test| 8487_1.txt|20181022T102331Z|    8487|             0|           1|Well I had the ch...|\n",
      "|       test| 6648_2.txt|20181022T102331Z|    6648|             0|           2|This is a movie t...|\n",
      "|       test| 2780_4.txt|20181022T102331Z|    2780|             0|           4|Dark Harvest is a...|\n",
      "|       test| 1789_2.txt|20181022T102331Z|    1789|             0|           2|A handful of nubi...|\n",
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.createDataFrame(pdf_read)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|datasettype|    filename| datetimecreated|reviewid|reviewpolarity|reviewrating|                text|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|      train|  9670_9.txt|20181022T102331Z|    9670|             1|           9|One of the best m...|\n",
      "|      train|  8747_7.txt|20181022T102331Z|    8747|             1|           7|*Possible Spoiler...|\n",
      "|      train| 5194_10.txt|20181022T102331Z|    5194|             1|          10|This is one of my...|\n",
      "|      train|  1725_8.txt|20181022T102331Z|    1725|             1|           8|Playing out as a ...|\n",
      "|      train|  6235_8.txt|20181022T102331Z|    6235|             1|           8|This is one of th...|\n",
      "|      train|  4900_8.txt|20181022T102331Z|    4900|             1|           8|This is a quirky ...|\n",
      "|      train| 10774_8.txt|20181022T102331Z|   10774|             1|           8|...about the impo...|\n",
      "|      train| 2515_10.txt|20181022T102331Z|    2515|             1|          10|Moon Child, starr...|\n",
      "|      train|   752_7.txt|20181022T102331Z|     752|             1|           7|Four teenage girl...|\n",
      "|      train| 7658_10.txt|20181022T102331Z|    7658|             1|          10|<br /><br />I hav...|\n",
      "|      train| 7109_10.txt|20181022T102331Z|    7109|             1|          10|First off, let's ...|\n",
      "|      train|  6720_9.txt|20181022T102331Z|    6720|             1|           9|Riding Giants<br ...|\n",
      "|      train|  411_10.txt|20181022T102331Z|     411|             1|          10|This movie is wel...|\n",
      "|      train|  4881_9.txt|20181022T102331Z|    4881|             1|           9|\"Kaabee\" depicts ...|\n",
      "|      train|   717_7.txt|20181022T102331Z|     717|             1|           7|huge Ramones fan....|\n",
      "|      train| 12447_8.txt|20181022T102331Z|   12447|             1|           8|There's a great d...|\n",
      "|      train|  340_10.txt|20181022T102331Z|     340|             1|          10|I loved This Movi...|\n",
      "|      train| 3295_10.txt|20181022T102331Z|    3295|             1|          10|Such a film of be...|\n",
      "|      train|10381_10.txt|20181022T102331Z|   10381|             1|          10|The previous revi...|\n",
      "|      train| 12096_8.txt|20181022T102331Z|   12096|             1|           8|I've watched this...|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the parquet file is good.\n",
    "ttl = 100000 #200\n",
    "file_pqt = os.path.join(base_path, (\"aclImdb_%s_raw.parquet\" % ttl))\n",
    "\n",
    "df_pqt = spark.read.parquet(file_pqt)\n",
    "\n",
    "# As needed.\n",
    "# df_pqt = df_pqt.drop('words')\n",
    "\n",
    "# Showing some observations (entries).\n",
    "df_pqt.persist()\n",
    "df_pqt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text to clean HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the secrets] of the universe. <br /><br />Unfortunately, \n",
      "the secrets of the universe Unfortunately \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    res = re.sub('<.*/>', '', text)\n",
    "    return  re.sub('[\\W]+', ' ', res)\n",
    "\n",
    "a_text = 'the secrets] of the universe. <br /><br />Unfortunately, '\n",
    "print(a_text)\n",
    "print(clean_html(a_text))\n",
    "\n",
    "from pyspark.sql import Row \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def preprocess(new_column_name, row):\n",
    "    data = row.asDict()\n",
    "    text = data['text']\n",
    "    \n",
    "    # Use a regex to clean HTML tags\n",
    "    text = clean_html(text)\n",
    "    data[new_column_name] = text\n",
    "    \n",
    "    return Row(**data)\n",
    "\n",
    "def transform_html_clean(df, new_column_name):\n",
    "    f = partial(preprocess, new_column_name)\n",
    "    return df.rdd.map(f).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"One of the best movies I ever saw was an Irish movie titled Philadelphia,Here I Come. I read the play before I saw the movie and loved them both. It's the story of a young man preparing to leave Ireland to go to America because he can't earn a living in Ireland. It is told both from the perspective of the young man(whom the other characters in the film can see) and another young man representing his uncensored thoughts and feelings., but who cannot be seen by the other characters in the film. It is a very sad movie, but deeply touching, and I would recommend this film to anyone who wants something to think about. I love any Irish movie, or almost any movie about Ireland, and any film that has the late Irish actor Donal McCann in it gets my vote.I would watch that man chew gum for 2 hours on screen, and unfortunately,I have.Terrible shame to have lost him so young.\", textclean='One of the best movies I ever saw was an Irish movie titled Philadelphia Here I Come I read the play before I saw the movie and loved them both It s the story of a young man preparing to leave Ireland to go to America because he can t earn a living in Ireland It is told both from the perspective of the young man whom the other characters in the film can see and another young man representing his uncensored thoughts and feelings but who cannot be seen by the other characters in the film It is a very sad movie but deeply touching and I would recommend this film to anyone who wants something to think about I love any Irish movie or almost any movie about Ireland and any film that has the late Irish actor Donal McCann in it gets my vote I would watch that man chew gum for 2 hours on screen and unfortunately I have Terrible shame to have lost him so young '),\n",
       " Row(text='*Possible Spoilers* Although done before (and better) in \\'Midnight Express\\' and \\'Return To Paradise\\', Brokedown Palace still strikes a chord with me.<br /><br />Here we have the tale of two young girls who travel through Thailand, and get arrested on charges of trafficking drugs. Was it Clare Danes\\' Alice? Was it Kate Beckinsale\\'s Darlene? Was it the handsome stranger whom they met on their journey? None of that really matters, for this is a tale of friendship and trust and the limits they can be stretched to. Throw in Bill Pullman as an unenthusiastic lawyer and Jacqueline Kim as his Thai bride (and better lawyer than he is) and we have a nice little story that holds the audience\\'s attention.<br /><br />Brokedown Palace is nothing extraordinary, or notorious for any reason - it is not an original concept, it doesn\\'t show sensationalised violence that leads to the wannabe-avant-garde crowd talking about it\\'s \"gritty realism\" or \"hard hitting truths\" - it is merely a good story with some fine performances. Bill Pullman is weakest with his lazy drawl and gravelly way of talking - I was quite bored with the character.<br /><br />Kate Beckinsale, while decent, is nothing spectacular. She acts well, but it\\'s not exactly a memorable performance.<br /><br />Jacqueline Kim however, is in fine form, crafting a likable and defined character where, really, there is not much to work with.<br /><br />But make no mistake - this is Clare Danes\\' movie. I\\'ve long been a fan of her work, and this is no change. She captivates in every scene she appears, and were it not for her, the film would probably stoop into boring fare. I particularly applaud her performance in the scene between her and Darlene\\'s father.<br /><br />The film also has a brilliant soundtrack.<br /><br />7 out of 10', textclean=' Possible Spoilers Although done before and better in Midnight Express and Return To Paradise Brokedown Palace still strikes a chord with me 7 out of 10')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the parquet data frame.\n",
    "df_pqt = transform_html_clean(df_pqt, 'textclean')\n",
    "df_pqt.select('text', 'textclean').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write: write\n",
      "writer: writer\n",
      "writing: write\n",
      "writers: writer\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['write','writer','writing','writers']\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}: {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Features Extractor Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['one', 'of', 'the', 'best', 'movies', 'i', 'ever', 'saw', 'was', 'an', 'irish', 'movie', 'titled', 'philadelphia', 'here', 'i', 'come', 'i', 'read', 'the', 'play', 'before', 'i', 'saw', 'the', 'movie', 'and', 'loved', 'them', 'both', 'it', 's', 'the', 'story', 'of', 'a', 'young', 'man', 'preparing', 'to', 'leave', 'ireland', 'to', 'go', 'to', 'america', 'because', 'he', 'can', 't', 'earn', 'a', 'living', 'in', 'ireland', 'it', 'is', 'told', 'both', 'from', 'the', 'perspective', 'of', 'the', 'young', 'man', 'whom', 'the', 'other', 'characters', 'in', 'the', 'film', 'can', 'see', 'and', 'another', 'young', 'man', 'representing', 'his', 'uncensored', 'thoughts', 'and', 'feelings', 'but', 'who', 'cannot', 'be', 'seen', 'by', 'the', 'other', 'characters', 'in', 'the', 'film', 'it', 'is', 'a', 'very', 'sad', 'movie', 'but', 'deeply', 'touching', 'and', 'i', 'would', 'recommend', 'this', 'film', 'to', 'anyone', 'who', 'wants', 'something', 'to', 'think', 'about', 'i', 'love', 'any', 'irish', 'movie', 'or', 'almost', 'any', 'movie', 'about', 'ireland', 'and', 'any', 'film', 'that', 'has', 'the', 'late', 'irish', 'actor', 'donal', 'mccann', 'in', 'it', 'gets', 'my', 'vote', 'i', 'would', 'watch', 'that', 'man', 'chew', 'gum', 'for', '2', 'hours', 'on', 'screen', 'and', 'unfortunately', 'i', 'have', 'terrible', 'shame', 'to', 'have', 'lost', 'him', 'so', 'young'])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "df_pqt = df_pqt.drop('words')\n",
    "            \n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words\")\n",
    "df_pqt = tokenizer.transform(df_pqt)\n",
    "\n",
    "df_pqt.select('words').take(1)\n",
    "\n",
    "# hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n",
    "# featurizedData = hashingTF.transform(wordsData)\n",
    "# # alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "# idfModel = idf.fit(featurizedData)\n",
    "# rescaledData = idfModel.transform(featurizedData)\n",
    "# for features_label in rescaledData.select(\"features\", \"label\").take(3):\n",
    "#     print(features_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'kitesurfing', 'long']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_text_cleaner(words):\n",
    "    words_clean = []\n",
    "    for a_w in words:\n",
    "        if not a_w in stopwords_bc.value:\n",
    "            words_clean.append(a_w)\n",
    "    \n",
    "    # Return the cleaned words.\n",
    "    return words_clean\n",
    "\n",
    "test_words = ['it', 'is', 'great', 'you', 'have', 'been', 'kitesurfing', 'that', 'long']\n",
    "row_text_cleaner(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          wordsclean|\n",
      "+--------------------+\n",
      "|[one, best, movie...|\n",
      "|[, possible, spoi...|\n",
      "|[one, favourite, ...|\n",
      "|[playing, sort, p...|\n",
      "|[one, best, fred,...|\n",
      "|[quirky, movie, b...|\n",
      "|[, importance, yo...|\n",
      "|[moon, child, sta...|\n",
      "|[four, teenage, g...|\n",
      "|[seen, movie, man...|\n",
      "|[first, let, star...|\n",
      "|[riding, giantsa,...|\n",
      "|[movie, well, don...|\n",
      "|[, kaabee, depict...|\n",
      "|[huge, ramones, f...|\n",
      "|[great, deal, mat...|\n",
      "|[loved, movie, sa...|\n",
      "|[film, beauty, ha...|\n",
      "|[previous, review...|\n",
      "|[watched, movie, ...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "def f(x):\n",
    "    data = x.asDict()\n",
    "    data['wordsclean'] = row_text_cleaner(x.words)\n",
    "    \n",
    "    # The purpose of ** is to give the ability to feed a function's arguments \n",
    "    # by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).\n",
    "    return Row(**data)\n",
    "\n",
    "# NOTE:\n",
    "# There is a need to store the result into df_pqt2 otherwise the\n",
    "# added words_clean added column does not show well if we store it in the same df_pqt when running:\n",
    "# df_pqt.select('words_clean').show()\n",
    "rdd_tmp = df_pqt.rdd.map(f)\n",
    "df_pqt = rdd_tmp.toDF()\n",
    "\n",
    "df_pqt.select('wordsclean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- textclean: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove intermediairy data not needed anymore.\n",
    "df_pqt.printSchema()\n",
    "# df_pqt = df_pqt.withColumnRenamed('words_clean', 'words')\n",
    "df_pqt = df_pqt.drop('textclean')\n",
    "df_pqt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF testing on small vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                             |rawFeatures                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[hi, i, heard, about, spark, and, i, love, spark, with, java,, i, read, a, lot, about, java, and, spark,, sparky!]|(5096,[56,338,565,677,1568,1722,2321,2455,2799,2905,3001,3202,4672,4673,4959],[1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0])|\n",
      "|[i, wish, java, could, use, case, classes]                                                                        |(5096,[449,1911,2085,2321,2799,4361,4390],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                    |\n",
      "|[logistic, regression, models, are, neat]                                                                         |(5096,[220,682,1449,1502,2111],[1.0,1.0,1.0,1.0,1.0])                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark and I love SPark with Java, I read a lot about java and spark, sparky!\"),\n",
    "    (0, \"I wish Java could use case classes\"),\n",
    "    (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "# This uses the hash and the modulo numFeatures to define a bucket where to put a word.\n",
    "# It is efficient as it does not store the vocabulary.\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5096)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.select('words', 'rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer test on small data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawfeatures\", vocabSize=70, minDF=1.0)\n",
    "model = cv.fit(featurizedData)\n",
    "result = model.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(rawfeatures=SparseVector(25, {0: 3.0, 1: 2.0, 2: 1.0, 3: 2.0, 4: 2.0, 5: 1.0, 7: 1.0, 9: 1.0, 12: 1.0, 14: 1.0, 17: 1.0, 19: 1.0, 20: 1.0, 22: 1.0, 23: 1.0}), words=['hi', 'i', 'heard', 'about', 'spark', 'and', 'i', 'love', 'spark', 'with', 'java,', 'i', 'read', 'a', 'lot', 'about', 'java', 'and', 'spark,', 'sparky!'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('rawfeatures', 'words').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "df_pqt = df_pqt.drop('featurestf')\n",
    "cv = CountVectorizer(inputCol=\"wordsclean\", outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "model_cv = cv.fit(df_pqt)\n",
    "df_pqt = model_cv.transform(df_pqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the corpus: 30000\n",
      "Excerpt of the vocabulary\n",
      "['film', 'one', 'like', 'good', 'time', 'see', 'would', 'even', 'really', 'story', 'well', 'great', 'bad', 'much', 'movies', 'people', 'first', 'get', 'made', 'watch', 'also', 'make', 'seen', 'could', 'films', 'way', 'think', 'ever', 'best', 'love', 'many', 'show', 'acting', 'never', 'characters', 'plot', 'better', 'life', 'two', 'know', 'little', 'say', '10', 'still', 'character', 'watching', 'man', 'end', 'something', 'go', 'years', 'funny', 'back', 'scenes', 'actors', 'director', 'thing', 'real', 'scene', 'old', 'though', 'find', 'lot', 'work', 'nothing', 'horror', 'another', 'actually', 'want', 'look', 'saw', 'every', 'new', 'going', 'makes', 'thought', 'cast', 'give', 'got', 'part', 'pretty', 'comedy', 'world', 'quite', 'must', 'series', 'big', 'us', 'long', 'things', 'original', 'however', 'take', 'enough', 'young', 'may', 'tv', 'always', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the corpus: %s\" % len(model_cv.vocabulary))\n",
    "print(\"Excerpt of the vocabulary\\n\" + str(model_cv.vocabulary[1:100]))\n",
    "\n",
    "# result.select('features').rdd.map(lambda x: print(x)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(30000, {0: 5.0, 1: 4.0, 2: 1.0, 6: 1.0, 7: 2.0, 10: 1.0, 15: 1.0, 20: 1.0, 23: 1.0, 27: 1.0, 28: 1.0, 29: 1.0, 30: 1.0, 35: 2.0, 47: 4.0, 49: 1.0, 50: 1.0, 67: 1.0, 71: 2.0, 95: 4.0, 103: 1.0, 110: 1.0, 111: 1.0, 124: 1.0, 134: 1.0, 156: 1.0, 158: 1.0, 179: 1.0, 188: 1.0, 190: 1.0, 227: 1.0, 242: 1.0, 315: 1.0, 355: 1.0, 361: 1.0, 420: 1.0, 426: 1.0, 449: 1.0, 462: 1.0, 467: 1.0, 469: 1.0, 474: 1.0, 615: 1.0, 695: 1.0, 1119: 1.0, 1335: 1.0, 1523: 1.0, 1799: 1.0, 1937: 1.0, 2203: 1.0, 2340: 3.0, 3113: 1.0, 3544: 3.0, 6156: 1.0, 6342: 1.0, 6429: 1.0, 7656: 1.0, 9227: 1.0, 9847: 1.0, 14618: 1.0, 20976: 1.0, 24773: 1.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.take(1)[0].featurestf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datasettype: string, datetimecreated: string, filename: string, reviewid: bigint, reviewpolarity: bigint, reviewrating: bigint, text: string, words: array<string>, wordsclean: array<string>, featurestf: vector, featuresidf: vector]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# Drop the column first.\n",
    "df_pqt = df_pqt.drop('featuresidf')\n",
    "\n",
    "# IDF uses a term frequency vector:\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=tfidf#pyspark.mllib.feature.IDF\n",
    "idf = IDF(inputCol=\"featurestf\", outputCol=\"featuresidf\")\n",
    "idfModel = idf.fit(df_pqt)\n",
    "df_pqt = idfModel.transform(df_pqt)\n",
    "\n",
    "df_pqt.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reviewpolarity=1, reviewrating=9, featuresidf=SparseVector(30000, {0: 3.1463, 1: 3.0069, 2: 0.8302, 6: 1.4339, 7: 2.9738, 10: 1.5891, 15: 1.7805, 20: 1.7708, 23: 1.831, 27: 1.9148, 28: 1.9333, 29: 1.9827, 30: 2.0803, 35: 4.1522, 47: 9.2522, 49: 2.2445, 50: 2.2376, 67: 2.423, 71: 4.9204, 95: 10.8737, 103: 2.7365, 110: 2.6847, 111: 2.7028, 124: 2.7596, 134: 2.8313, 156: 2.8961, 158: 2.9783, 179: 3.0713, 188: 3.1508, 190: 3.1127, 227: 3.2655, 242: 3.2597, 315: 3.4712, 355: 3.5227, 361: 3.5649, 420: 3.7508, 426: 3.7406, 449: 3.781, 462: 3.7679, 467: 3.7889, 469: 3.7881, 474: 3.8286, 615: 4.041, 695: 4.1734, 1119: 4.5933, 1335: 4.8359, 1523: 4.9533, 1799: 5.1534, 1937: 5.2401, 2203: 5.3517, 2340: 17.0169, 3113: 5.8025, 3544: 18.5552, 6156: 6.7089, 6342: 6.6609, 6429: 6.7255, 7656: 6.9486, 9227: 7.1822, 9847: 7.3233, 14618: 7.8754, 20976: 8.6226, 24773: 8.8739}), text=\"One of the best movies I ever saw was an Irish movie titled Philadelphia,Here I Come. I read the play before I saw the movie and loved them both. It's the story of a young man preparing to leave Ireland to go to America because he can't earn a living in Ireland. It is told both from the perspective of the young man(whom the other characters in the film can see) and another young man representing his uncensored thoughts and feelings., but who cannot be seen by the other characters in the film. It is a very sad movie, but deeply touching, and I would recommend this film to anyone who wants something to think about. I love any Irish movie, or almost any movie about Ireland, and any film that has the late Irish actor Donal McCann in it gets my vote.I would watch that man chew gum for 2 hours on screen, and unfortunately,I have.Terrible shame to have lost him so young.\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.select('reviewpolarity', \"reviewrating\", \"featuresidf\", 'text').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model using N-fold stratified cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'being',\n",
       " 'off',\n",
       " 'haven',\n",
       " 'few',\n",
       " 'up',\n",
       " 'while',\n",
       " \"it's\",\n",
       " 'those',\n",
       " 'is',\n",
       " 'same',\n",
       " 'mustn',\n",
       " 'in',\n",
       " 've',\n",
       " 'hers',\n",
       " 'all',\n",
       " 'above',\n",
       " 'very',\n",
       " 'they']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = list(set(stopwords.words('english')))\n",
    "\n",
    "stopwords_set[1:20]\n",
    "# stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Create a test df.\n",
    "df0 = transform_html_clean(df_pqt, 'textclean')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_test\", stopWords=stopwords_set)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover])\n",
    "\n",
    "len(pipeline.fit(df0).transform(df0).head().words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45043, 4957)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the df into train and test\n",
    "df_training, df_test = df_pqt.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "df_training.count(), df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|reviewpolarity|count|\n",
      "+--------------+-----+\n",
      "|             0|22625|\n",
      "|             1|22418|\n",
      "+--------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|reviewpolarity|count|\n",
      "+--------------+-----+\n",
      "|             0| 2375|\n",
      "|             1| 2582|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training.groupBy('reviewpolarity').count().show()\n",
    "df_test.groupBy('reviewpolarity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_training.drop('words')\n",
    "df_training = df_training.drop('featurestf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Based on Spark doc\n",
    "# https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)\n",
    "\n",
    "# The idea is to create a features vector from a list of words.\n",
    "\n",
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Or 2) use the Term Frequency - Inverse Document Frequency.\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
    "\n",
    "# Define the criteria ranges.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100, 50000, 200000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Define the cross validation runner.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "df_training_tmp = df_training.withColumnRenamed('reviewpolarity', 'label')\n",
    "df_training_ppl = transform_html_clean(df_training_tmp, 'textclean')\n",
    "\n",
    "# Train the model.\n",
    "cvModel = crossval.fit(df_training_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9306365250129929,\n",
       " 0.9192822909849401,\n",
       " 0.9305837407495565,\n",
       " 0.9192582717589457,\n",
       " 0.9306982633744618,\n",
       " 0.9190437984787838]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0.9957316580155462\n"
     ]
    }
   ],
   "source": [
    "df_training_pip = model_best.transform(df_training_ppl)\n",
    "eval_val = evaluator.evaluate(df_training_pip)\n",
    "print(evaluator.isLargerBetter())\n",
    "print(eval_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1, probability=DenseVector([0.1814, 0.8186]), prediction=1.0, words=['brilliant', 'acting', 'lesley', 'ann', 'warren', 'best', 'dramatic', 'hobo', 'lady', 'ever', 'seen', 'love', 'scenes', 'clothes', 'warehouse', 'second', 'none', 'corn', 'face', 'classic', 'good', 'anything', 'blazing', 'saddles', 'take', 'lawyers', 'also', 'superb', 'accused', 'turncoat', 'selling', 'boss', 'dishonest', 'lawyer', 'pepto', 'bolt', 'shrugs', 'indifferently', 'lawyer', 'says', 'three', 'funny', 'words', 'jeffrey', 'tambor', 'favorite', 'later', 'larry', 'sanders', 'show', 'fantastic', 'mad', 'millionaire', 'wants', 'crush', 'ghetto', 'character', 'malevolent', 'usual', 'hospital', 'scene', 'scene', 'homeless', 'invade', 'demolition', 'site', 'time', 'classics', 'look', 'legs', 'scene', 'two', 'big', 'diggers', 'fighting', 'one', 'bleeds', 'movie', 'gets', 'better', 'time', 'see', 'quite', 'often']),\n",
       " Row(label=1, probability=DenseVector([0.1532, 0.8468]), prediction=1.0, words=['easily', 'underrated', 'film', 'inn', 'brooks', 'cannon', 'sure', 'flawed', 'give', 'realistic', 'view', 'homelessness', 'unlike', 'say', 'citizen', 'kane', 'gave', 'realistic', 'view', 'lounge', 'singers', 'titanic', 'gave', 'realistic', 'view', 'italians', 'idiots', 'many', 'jokes', 'fall', 'flat', 'still', 'film', 'lovable', 'way', 'many', 'comedies', 'pull', 'story', 'traditionally', 'reviled', 'members', 'society', 'truly', 'impressive', 'fisher', 'king', 'crap', 'either', 'complaint', 'brooks', 'cast', 'someone', 'else', 'lead', 'love', 'mel', 'director', 'writer', 'much', 'lead'])]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_pip.filter(df_training_pip.label == df_training_pip.prediction) \\\n",
    "    .select('label', 'probability', 'prediction', 'words').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the cross validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop('featurestf')\n",
    "df_test = df_test.drop('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.385, 0.615]), label=1, prediction=1.0, features=SparseVector(30000, {1: 0.7496, 91: 2.6101, 105: 2.7464, 129: 2.7892, 162: 3.0012, 167: 3.0567, 192: 3.2199, 254: 3.27, 264: 3.2924, 304: 3.4265, 339: 3.4848, 367: 3.5561, 443: 3.7608, 514: 3.9515, 539: 3.8932, 625: 4.0927, 815: 4.6309, 1000: 4.4771, 1212: 4.6968, 1503: 4.9224, 1625: 5.2182, 1632: 5.3172, 1937: 5.5059, 2187: 5.528, 2455: 5.5336, 2660: 5.6155, 2665: 5.7114, 2861: 6.0333, 3027: 5.9618, 3212: 6.1615, 3321: 5.9969, 3554: 5.9032, 3967: 6.052, 4142: 6.1003, 5727: 6.9542, 6874: 7.16, 6910: 6.7642, 8610: 7.0778, 9627: 7.3832, 10766: 7.771, 11138: 7.771, 12134: 7.771, 12480: 7.825, 20053: 8.5182, 25955: 8.9236}), words=['night', 'listener', '2006', '1', '2', 'robin', 'williams', 'toni', 'collette', 'bobby', 'cannavale', 'rory', 'culkin', 'joe', 'morton', 'sandra', 'oh', 'john', 'cullum', 'lisa', 'emery', 'becky', 'ann', 'baker', 'dir', 'patrick', 'stettner', 'however', 'film', 'runs', 'gas', 'eventually', 'becomes', 'bit', 'repetitive', 'predictable', 'despite', 'finely', 'directed', 'piece', 'hoodwink', 'mystery', 'stettner', 'pays', 'listen', 'inner', 'voice', 'careful', 'hope']),\n",
       " Row(probability=DenseVector([0.1021, 0.8979]), label=1, prediction=1.0, features=SparseVector(30000, {0: 6.3042, 2: 1.6595, 3: 1.0948, 4: 1.28, 5: 1.3946, 12: 3.3871, 15: 1.7886, 16: 1.7821, 24: 1.8293, 27: 1.9151, 29: 1.9812, 34: 2.0313, 45: 2.2276, 75: 2.4679, 92: 2.7777, 94: 2.6564, 103: 2.6886, 111: 2.6876, 117: 2.7299, 125: 2.7805, 136: 2.8059, 139: 2.8641, 147: 2.9678, 155: 2.9887, 159: 3.0012, 167: 3.0567, 168: 2.9998, 170: 2.9936, 172: 3.0478, 221: 3.159, 233: 3.1925, 405: 3.6961, 431: 3.7233, 446: 3.7928, 516: 3.8547, 539: 3.8932, 551: 3.8899, 714: 4.2022, 825: 4.3803, 883: 4.375, 929: 4.4294, 1008: 4.4654, 1365: 4.8321, 1414: 4.8983, 1477: 4.8835, 1747: 5.095, 2455: 5.5336, 3474: 5.9363, 3547: 5.9447, 4170: 6.1203, 9850: 7.2814, 12480: 7.825, 20036: 8.4128}), words=['night', 'listener', 'probably', 'one', 'william', 'best', 'roles', 'makes', 'interesting', 'character', 'somewhat', 'odd', 'different', 'movie', 'guarantee', 'never', 'seen', 'kind', 'movie', 'people', 'maybe', 'like', 'slow', 'pacing', 'movie', 'think', 'great', 'plus', 'movie', 'definitely', 'one', 'top', 'movies', 'come', 'year', '2006', 'intriguing', 'performance', 'movie', 'great', 'content', 'dramatic', 'feeling', 'americanized', 'movie', 'neither', 'predictable', 'movie', 'feel', 'movie', 'secrets', 'hard', 'time', 'determine', 'outcome', 'may', 'excellent', 'movie', 'everything', 'hell', 'damn', 'good', 'original', 'movie'])]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data.\n",
    "df_test_tmp = df_test.withColumnRenamed('reviewpolarity','label')\n",
    "df_test_ppl = transform_html_clean(df_test_tmp, 'textclean')\n",
    "\n",
    "# Make prediction.\n",
    "df_test_res = model_best.transform(df_test_ppl)\n",
    "df_test_res.select('probability', 'label','prediction', 'features', 'words').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9332059195238258\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(df_test_res))\n",
    "# df_test_res.filter(df_test_res.label == df_test_res.prediction) \\\n",
    "#     .select('label', 'probability', 'prediction', 'features', 'words').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21890.   735.]\n",
      " [  698. 21720.]]\n",
      "\n",
      "0.9688643054688197\n",
      "0.03248618784530387\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd_training_pip = df_training_pip.select('prediction', 'label').rdd.map(lambda row: (row[0], float(row[1])))\n",
    "rdd_training_pip.take(2)\n",
    "\n",
    "# print(rdd_training_pip.toDF().toPandas().shape)\n",
    "\n",
    "metrics = MulticlassMetrics(rdd_training_pip)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print()\n",
    "print(metrics.truePositiveRate(1.0))\n",
    "print(metrics.falsePositiveRate(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiver Operating Characteristics (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_46bd867292f80135c6d4"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                 FPR|                 TPR|\n",
      "+--------------------+--------------------+\n",
      "|                 0.0|                 0.0|\n",
      "|                 0.0|0.019894727451155322|\n",
      "|                 0.0| 0.03987866892675528|\n",
      "|                 0.0| 0.05986261040235525|\n",
      "|                 0.0| 0.07980194486573289|\n",
      "|                 0.0| 0.09974127932911053|\n",
      "|                 0.0| 0.11968061379248818|\n",
      "|                 0.0| 0.13970916228031047|\n",
      "|                 0.0| 0.15969310375591042|\n",
      "|                 0.0|  0.1797216522437327|\n",
      "|                 0.0|  0.1997948077437773|\n",
      "|                 0.0| 0.21973414220715495|\n",
      "|                 0.0| 0.23891515746275316|\n",
      "|                 0.0|  0.2588990989383531|\n",
      "|                 0.0| 0.27906146846284235|\n",
      "|                 0.0|   0.299134623962887|\n",
      "|                 0.0| 0.31911856543848693|\n",
      "|4.419889502762431E-5| 0.33901329288964227|\n",
      "|4.419889502762431E-5| 0.35904184137746453|\n",
      "|4.419889502762431E-5| 0.37911499687750916|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionSummary\n",
    "\n",
    "# Get the Logistic regression model to get the summary.\n",
    "summary = cvModel.bestModel.stages[-1].summary\n",
    "summary.roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# As defined by IPython matplotlib kernel\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/plotting.html#id1\n",
    "%matplotlib inline\n",
    "\n",
    "aPlt = summary.roc.toPandas().plot(x='FPR', y='TPR', colormap='winter_r')\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], linestyle='--', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent for online and out-of-core learning Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 entries from the CSV file\n"
     ]
    }
   ],
   "source": [
    "# Use the df_csv loaded earlier.\n",
    "print(\"%s entries from the CSV file\" % df_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator to load the data from the file simulating a streaming.\n",
    "ttl = 100000\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "def stream_doc():\n",
    "    with open(file_csv, 'r', encoding='utf-8') as csv:\n",
    "        # skip header.\n",
    "        next(csv)\n",
    "        \n",
    "        for line in csv:\n",
    "            cells = line.split(',')\n",
    "#             datasettype,filename,datetimecreated,reviewid,reviewpolarity,reviewrating,text = cells[0], \\\n",
    "#             cells[1], cells[2], cells[3], cells[4], cells[5], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            filename,reviewpolarity,text = cells[1], cells[4], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            yield filename,reviewpolarity,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1821_4.txt', '0', '\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It\\'s worth seeing for their scenes- and Rickman\\'s scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\"')\n",
      "('9487_1.txt', '0', 'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.')\n"
     ]
    }
   ],
   "source": [
    "generator = stream_doc()\n",
    "print(next(generator))\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a number of documents (id, text) and their label from the doc stream.\n",
    "def get_mini_batch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            filename,reviewpolarity,text = next(doc_stream)\n",
    "            docs.append([filename, text])\n",
    "            y.append(int(reviewpolarity))\n",
    "    except StopIteration:\n",
    "        return docs, y\n",
    "    \n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['1821_4.txt',\n",
       "   '\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It\\'s worth seeing for their scenes- and Rickman\\'s scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\"'],\n",
       "  ['9487_1.txt',\n",
       "   'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.']],\n",
       " [0, 0])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the function we just wrote.\n",
    "get_mini_batch(stream_doc(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of SciKit Learn data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "# from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "boston = load_boston()\n",
    "print(type(boston.data[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)\n",
    "\n",
    "# The idea is to create a features vector from a list of words.\n",
    "\n",
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF])\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hujol/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 1\n",
      "range 2\n",
      "range 3\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "score: 1.000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5, shuffle=True)\n",
    "\n",
    "# Get the X and y labels.\n",
    "def generate_X_y_labels(size):\n",
    "    data_batch, labels_batch = get_mini_batch(data_stream, size)\n",
    "    \n",
    "    if not data_batch: return np.empty(), np.empty()\n",
    "    \n",
    "    df_batch = spark.createDataFrame(data_batch, ('id', 'text'))\n",
    "\n",
    "    # Data cleansing.\n",
    "    df_batch_clean = transform_html_clean(df_batch, 'textclean')\n",
    "    df_training_tmp = df_batch_clean.withColumnRenamed('reviewpolarity', 'label')\n",
    "\n",
    "    # Run the tokenizer and remover pipeline.\n",
    "    m_pip = pipeline.fit(df_training_tmp)\n",
    "    df_pip_batch = m_pip.transform(df_training_tmp)\n",
    "    # Update the SGD regression weights.\n",
    "\n",
    "    # Let's get the right shape for the SparseVector data into numpy arrays.\n",
    "    series = df_pip_batch.toPandas()['features'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)\n",
    "    X = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "    y_labels =  np.array(labels_batch)\n",
    "\n",
    "    return X, y_labels\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# print(X[:])\n",
    "# print(y_labels[1:10])\n",
    "\n",
    "# Simulating a streaming\n",
    "data_stream = stream_doc()\n",
    "\n",
    "# Train the 45000 data from the entire data set.\n",
    "for i in range(4):\n",
    "    print(\"range %i\" % i)\n",
    "    X_train, y_labels_train = generate_X_y_labels(1000)\n",
    "    if not len(X_train): break\n",
    "        \n",
    "    model_sgd = clf.partial_fit(X_train, y_labels_train, classes=classes)\n",
    "\n",
    "# Test on the last 5000 entries.\n",
    "X_test, y_labels_test = generate_X_y_labels(5000)\n",
    "\n",
    "print(X_test)\n",
    "if len(X_test):\n",
    "    print(\"\\nscore: %.3f\" % model_sgd.score(X_test, y_labels_test))\n",
    "else:\n",
    "    print('No data')\n",
    "\n",
    "# Train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark V2.3.2 (Local)",
   "language": "python",
   "name": "pyspark-2.3.2-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
