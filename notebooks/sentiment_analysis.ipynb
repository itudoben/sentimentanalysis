{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis of a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base path for the ACL IMDB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the reviews into data frames.\n",
    "base_path = \"/Users/hujol/Projects/advanced_analytics_spark/data/aclImdb/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Python system path to find our modules.\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our modules.\n",
    "import file_loader as fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in a parquet file.\n",
    "file_pqt = fl.load_data(base_path, 252, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README                     \u001b[34maclImdb_211_raw.parquet\u001b[m\u001b[m\n",
      "aclImdb_100000.csv         \u001b[34maclImdb_250_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_100000_raw.parquet\u001b[m\u001b[m \u001b[34maclImdb_251_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_10000_raw.parquet\u001b[m\u001b[m  \u001b[34maclImdb_252_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_1000_raw.parquet\u001b[m\u001b[m   \u001b[34maclImdb_300_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_100_raw.parquet\u001b[m\u001b[m    \u001b[34maclImdb_50000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_20000_raw.parquet\u001b[m\u001b[m  imdb.vocab\n",
      "\u001b[34maclImdb_2000_raw.parquet\u001b[m\u001b[m   imdbEr.txt\n",
      "\u001b[34maclImdb_200_raw.parquet\u001b[m\u001b[m    \u001b[34mtest\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_210_raw.parquet\u001b[m\u001b[m    \u001b[34mtrain\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls {base_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "# Get the data as Pandas data frame.\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Re index to shuffle the data before saving it.\n",
    "pdf = pdf.reindex(np.random.permutation(pdf.index))\n",
    "pdf.to_csv(file_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV file for checking data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ttl = 100000\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "pdf_read = pd.read_csv(file_csv, encoding='utf-8')\n",
    "pdf_read[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = spark.createDataFrame(pdf_read)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|datasettype|    filename| datetimecreated|reviewid|reviewpolarity|reviewrating|                text|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|       test|11813_10.txt|20181024T150644Z|   11813|             1|          10|The Cure is a fan...|\n",
      "|       test|   835_8.txt|20181024T150644Z|     835|             1|           8|The original Fema...|\n",
      "|       test|  4245_8.txt|20181024T150644Z|    4245|             1|           8|remember back whe...|\n",
      "|       test| 11856_7.txt|20181024T150644Z|   11856|             1|           7|Sophisticated sex...|\n",
      "|       test|  6133_8.txt|20181024T150644Z|    6133|             1|           8|I stumbled upon t...|\n",
      "|       test| 10167_9.txt|20181024T150644Z|   10167|             1|           9|A film that tends...|\n",
      "|       test|  903_10.txt|20181024T150644Z|     903|             1|          10|This is a well do...|\n",
      "|       test|  1466_8.txt|20181024T150644Z|    1466|             1|           8|A strange relatio...|\n",
      "|       test|  6176_8.txt|20181024T150644Z|    6176|             1|           8|This is a brillia...|\n",
      "|       test| 5124_10.txt|20181024T150644Z|    5124|             1|          10|i read the book b...|\n",
      "|       test| 2807_10.txt|20181024T150644Z|    2807|             1|          10|I played Sam (the...|\n",
      "|       test|  4038_9.txt|20181024T150644Z|    4038|             1|           9|\"Snow Queen\" is b...|\n",
      "|       test| 10961_9.txt|20181024T150644Z|   10961|             1|           9|Watching John Cas...|\n",
      "|       test|  2835_7.txt|20181024T150644Z|    2835|             1|           7|This is a luminou...|\n",
      "|       test| 7695_10.txt|20181024T150644Z|    7695|             1|          10|I was @ 13 yrs of...|\n",
      "|       test|  9919_9.txt|20181024T150644Z|    9919|             1|           9|Title: Opera (198...|\n",
      "|       test| 12091_8.txt|20181024T150644Z|   12091|             1|           8|'Crossing the Bri...|\n",
      "|       test| 6644_10.txt|20181024T150644Z|    6644|             1|          10|A gruelling watch...|\n",
      "|       test| 9491_10.txt|20181024T150644Z|    9491|             1|          10|I bought my first...|\n",
      "|       test| 6115_10.txt|20181024T150644Z|    6115|             1|          10|A brilliant film ...|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the parquet file is good.\n",
    "df_pqt = spark.read.parquet(file_pqt)\n",
    "\n",
    "# As needed.\n",
    "# df_pqt = df_pqt.drop('words')\n",
    "\n",
    "# Showing some observations (entries).\n",
    "df_pqt.persist()\n",
    "df_pqt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text to clean HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the secrets] of the universe. <br /><br />Unfortunately, \n",
      "the secrets of the universe Unfortunately \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_html(text):\n",
    "    res = re.sub('<.*/>', '', text)\n",
    "    return  re.sub('[\\W]+', ' ', res)\n",
    "\n",
    "a_text = 'the secrets] of the universe. <br /><br />Unfortunately, '\n",
    "print(a_text)\n",
    "print(clean_html(a_text))\n",
    "\n",
    "from pyspark.sql import Row \n",
    "\n",
    "from functools import partial\n",
    "\n",
    "def preprocess(new_column_name, row):\n",
    "    data = row.asDict()\n",
    "    text = data['text']\n",
    "    \n",
    "    # Use a regex to clean HTML tags\n",
    "    text = clean_html(text)\n",
    "    data[new_column_name] = text\n",
    "    \n",
    "    return Row(**data)\n",
    "\n",
    "def transform_html_clean(df, new_column_name):\n",
    "    f = partial(preprocess, new_column_name)\n",
    "    return df.rdd.map(f).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"Latcho Drom, or Safe Journey, is the second film in Tony Gatlif's trilogy of the Romany people. The film is a visual depiction and historical record of Romany life in European and Middle Eastern countries. Even though the scenes are mostly planned, rehearsed, and staged there is not a conventional story line and the dialog does not explain activities from scene to scene. Instead, the film allows the viewer to have sometimes a glimpse, sometimes a more in-depth view of these people during different eras and in different countries, ranging from India, Egypt, Romania, Hungary, Slovakia, France, and Spain.<br /><br />The importance of music in Romany culture is clearly expressed throughout the film. It is a vital part of every event and an important means of communication. Everything they do is expressed with music. Dance is another important activity. Like Romany music, it is specialized and deeply personal, something they alone know how to do correctly. We are provided glimpses into their everyday activities, but the film is not a detailed study of their lives. Rather, it is a testament to their culture, focusing on the music and dance they have created and which have made them unique.<br /><br />Mr. Gatlif portrays the nomadic groups in a positive way. However, we also witness the rejection, distrust, and alienation they receive from the non-Romany population. It seems that the culture they have developed over countless generations, and inspired from diverse countries, will fade into oblivion because conventional society has no place for nomadic ways.<br /><br />The other films in the trilogy are Les Princes (1983) and Gadjo Dilo (1998).\", textclean='Latcho Drom or Safe Journey is the second film in Tony Gatlif s trilogy of the Romany people The film is a visual depiction and historical record of Romany life in European and Middle Eastern countries Even though the scenes are mostly planned rehearsed and staged there is not a conventional story line and the dialog does not explain activities from scene to scene Instead the film allows the viewer to have sometimes a glimpse sometimes a more in depth view of these people during different eras and in different countries ranging from India Egypt Romania Hungary Slovakia France and Spain The other films in the trilogy are Les Princes 1983 and Gadjo Dilo 1998 ')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the parquet data frame.\n",
    "df_pqt = transform_html_clean(df_pqt, 'textclean')\n",
    "df_pqt.select('text', 'textclean').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write: write\n",
      "writer: writer\n",
      "writing: write\n",
      "writers: writer\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['write','writer','writing','writers']\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}: {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Features Extractor Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['latcho', 'drom', 'or', 'safe', 'journey', 'is', 'the', 'second', 'film', 'in', 'tony', 'gatlif', 's', 'trilogy', 'of', 'the', 'romany', 'people', 'the', 'film', 'is', 'a', 'visual', 'depiction', 'and', 'historical', 'record', 'of', 'romany', 'life', 'in', 'european', 'and', 'middle', 'eastern', 'countries', 'even', 'though', 'the', 'scenes', 'are', 'mostly', 'planned', 'rehearsed', 'and', 'staged', 'there', 'is', 'not', 'a', 'conventional', 'story', 'line', 'and', 'the', 'dialog', 'does', 'not', 'explain', 'activities', 'from', 'scene', 'to', 'scene', 'instead', 'the', 'film', 'allows', 'the', 'viewer', 'to', 'have', 'sometimes', 'a', 'glimpse', 'sometimes', 'a', 'more', 'in', 'depth', 'view', 'of', 'these', 'people', 'during', 'different', 'eras', 'and', 'in', 'different', 'countries', 'ranging', 'from', 'india', 'egypt', 'romania', 'hungary', 'slovakia', 'france', 'and', 'spain', 'the', 'other', 'films', 'in', 'the', 'trilogy', 'are', 'les', 'princes', '1983', 'and', 'gadjo', 'dilo', '1998'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "df_pqt = df_pqt.drop('words')\n",
    "            \n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words\")\n",
    "df_pqt = tokenizer.transform(df_pqt)\n",
    "\n",
    "df_pqt.select('words').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'kitesurfing', 'long']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_text_cleaner(words):\n",
    "    words_clean = []\n",
    "    for a_w in words:\n",
    "        if not a_w in stopwords_bc.value:\n",
    "            words_clean.append(a_w)\n",
    "    \n",
    "    # Return the cleaned words.\n",
    "    return words_clean\n",
    "\n",
    "test_words = ['it', 'is', 'great', 'you', 'have', 'been', 'kitesurfing', 'that', 'long']\n",
    "row_text_cleaner(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          wordsclean|\n",
      "+--------------------+\n",
      "|[latcho, drom, sa...|\n",
      "|[another, pleasan...|\n",
      "|[admit, awe, sea,...|\n",
      "|[film, shows, ser...|\n",
      "|[co, scripted, wi...|\n",
      "|[okay, let, start...|\n",
      "|[serious, film, p...|\n",
      "|[film, chock, ful...|\n",
      "|[ok, following, r...|\n",
      "|[rajkumar, santos...|\n",
      "|[think, pauly, sh...|\n",
      "|[, north, south, ...|\n",
      "|[, whoops, looks,...|\n",
      "|[went, film, thin...|\n",
      "|[anyone, spent, t...|\n",
      "|[, last, time, la...|\n",
      "|[british, documen...|\n",
      "|[upon, time, trom...|\n",
      "|[really, liked, m...|\n",
      "|[beguiled, one, e...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "def f(x):\n",
    "    data = x.asDict()\n",
    "    data['wordsclean'] = row_text_cleaner(x.words)\n",
    "    \n",
    "    # The purpose of ** is to give the ability to feed a function's arguments \n",
    "    # by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).\n",
    "    return Row(**data)\n",
    "\n",
    "# NOTE:\n",
    "# There is a need to store the result into df_pqt2 otherwise the\n",
    "# added words_clean added column does not show well if we store it in the same df_pqt when running:\n",
    "# df_pqt.select('words_clean').show()\n",
    "rdd_tmp = df_pqt.rdd.map(f)\n",
    "df_pqt = rdd_tmp.toDF()\n",
    "\n",
    "df_pqt.select('wordsclean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- textclean: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove intermediairy data not needed anymore.\n",
    "df_pqt.printSchema()\n",
    "# df_pqt = df_pqt.withColumnRenamed('words_clean', 'words')\n",
    "df_pqt = df_pqt.drop('textclean')\n",
    "df_pqt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF testing on small vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                             |rawFeatures                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[hi, i, heard, about, spark, and, i, love, spark, with, java,, i, read, a, lot, about, java, and, spark,, sparky!]|(5096,[56,338,565,677,1568,1722,2321,2455,2799,2905,3001,3202,4672,4673,4959],[1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0])|\n",
      "|[i, wish, java, could, use, case, classes]                                                                        |(5096,[449,1911,2085,2321,2799,4361,4390],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                    |\n",
      "|[logistic, regression, models, are, neat]                                                                         |(5096,[220,682,1449,1502,2111],[1.0,1.0,1.0,1.0,1.0])                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark and I love SPark with Java, I read a lot about java and spark, sparky!\"),\n",
    "    (0, \"I wish Java could use case classes\"),\n",
    "    (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "# This uses the hash and the modulo numFeatures to define a bucket where to put a word.\n",
    "# It is efficient as it does not store the vocabulary.\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5096)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.select('words', 'rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer test on small data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawfeatures\", vocabSize=70, minDF=1.0)\n",
    "model = cv.fit(featurizedData)\n",
    "result = model.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(rawfeatures=SparseVector(25, {0: 3.0, 1: 1.0, 2: 2.0, 3: 2.0, 4: 2.0, 5: 1.0, 7: 1.0, 10: 1.0, 12: 1.0, 13: 1.0, 14: 1.0, 17: 1.0, 19: 1.0, 21: 1.0, 24: 1.0}), words=['hi', 'i', 'heard', 'about', 'spark', 'and', 'i', 'love', 'spark', 'with', 'java,', 'i', 'read', 'a', 'lot', 'about', 'java', 'and', 'spark,', 'sparky!'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('rawfeatures', 'words').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "df_pqt = df_pqt.drop('featurestf')\n",
    "cv = CountVectorizer(inputCol=\"wordsclean\", outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "model_cv = cv.fit(df_pqt)\n",
    "df_pqt = model_cv.transform(df_pqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the corpus: 4680\n",
      "Excerpt of the vocabulary\n",
      "['film', 'one', 'like', 'see', 'time', 'really', 'good', 'would', 'people', 'even', 'great', 'watch', 'bad', 'movies', 'get', 'well', 'think', 'seen', 'films', 'first', 'story', 'could', 'much', 'also', 'made', 'ever', 'plot', 'two', 'acting', 'show', 'many', 'make', 'never', 'know', 'way', 'man', 'love', 'characters', 'say', 'back', 'original', 'something', 'want', 'still', 'things', 'going', 'little', 'watching', 'best', 'dvd', 'better', 'take', 'right', 'character', 'life', 'real', 'comedy', 'funny', 'horror', 'director', 'actors', '10', 'thought', 'cast', 'work', 'nothing', 'look', 'another', 'long', 'enough', 'every', '2', 'anyone', 'effects', 'end', 'go', 'watched', 'music', 'though', 'sound', 'book', 'far', 'lot', 'thing', 'short', 'day', 'may', 'role', 'job', 'minutes', 'different', 'classic', 'world', 'point', 'fan', 'read', 'worth', 'saw', 'put']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the corpus: %s\" % len(model_cv.vocabulary))\n",
    "print(\"Excerpt of the vocabulary\\n\" + str(model_cv.vocabulary[1:100]))\n",
    "\n",
    "# result.select('features').rdd.map(lambda x: print(x)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(4680, {1: 3.0, 9: 2.0, 10: 1.0, 19: 1.0, 21: 1.0, 55: 1.0, 79: 1.0, 91: 2.0, 100: 1.0, 130: 1.0, 141: 2.0, 149: 1.0, 160: 1.0, 192: 1.0, 360: 1.0, 386: 1.0, 407: 1.0, 785: 1.0, 787: 1.0, 810: 1.0, 837: 2.0, 842: 1.0, 945: 1.0, 948: 1.0, 968: 1.0, 998: 1.0, 1068: 1.0, 1162: 1.0, 1495: 1.0, 1499: 2.0, 1540: 2.0, 1593: 2.0, 1705: 1.0, 1760: 1.0, 1870: 1.0, 1911: 1.0, 1917: 1.0, 2239: 1.0, 2290: 1.0, 2429: 1.0, 2759: 1.0, 2904: 1.0, 2984: 1.0, 2998: 1.0, 3190: 1.0, 3222: 1.0, 3537: 1.0, 3714: 1.0, 3868: 1.0, 4105: 1.0, 4156: 1.0, 4172: 1.0, 4193: 1.0, 4206: 1.0, 4271: 1.0, 4290: 1.0, 4656: 1.0, 4671: 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.take(1)[0].featurestf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datasettype: string, datetimecreated: string, filename: string, reviewid: bigint, reviewpolarity: bigint, reviewrating: bigint, text: string, words: array<string>, wordsclean: array<string>, featurestf: vector, featuresidf: vector]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# Drop the column first.\n",
    "df_pqt = df_pqt.drop('featuresidf')\n",
    "\n",
    "# IDF uses a term frequency vector:\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=tfidf#pyspark.mllib.feature.IDF\n",
    "idf = IDF(inputCol=\"featurestf\", outputCol=\"featuresidf\")\n",
    "idfModel = idf.fit(df_pqt)\n",
    "df_pqt = idfModel.transform(df_pqt)\n",
    "\n",
    "df_pqt.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reviewpolarity=1, reviewrating=7, featuresidf=SparseVector(4680, {1: 2.3767, 9: 3.286, 10: 1.4446, 19: 1.8601, 21: 1.719, 55: 2.2655, 79: 2.3609, 91: 5.168, 100: 2.584, 130: 2.7175, 141: 5.7434, 149: 2.7175, 160: 2.8717, 192: 2.8717, 360: 3.4107, 386: 3.4107, 407: 3.4107, 785: 3.7471, 787: 3.9703, 810: 4.258, 837: 8.5159, 842: 3.9703, 945: 3.9703, 948: 3.9703, 968: 4.258, 998: 3.9703, 1068: 4.258, 1162: 4.258, 1495: 4.258, 1499: 9.3269, 1540: 9.3269, 1593: 9.3269, 1705: 4.258, 1760: 4.258, 1870: 4.6634, 1911: 4.6634, 1917: 4.6634, 2239: 4.6634, 2290: 4.6634, 2429: 4.6634, 2759: 4.6634, 2904: 4.6634, 2984: 4.6634, 2998: 4.6634, 3190: 4.6634, 3222: 4.6634, 3537: 4.6634, 3714: 4.6634, 3868: 4.6634, 4105: 4.6634, 4156: 4.6634, 4172: 4.6634, 4193: 4.6634, 4206: 4.6634, 4271: 4.6634, 4290: 4.6634, 4656: 4.6634, 4671: 4.6634}), text=\"Latcho Drom, or Safe Journey, is the second film in Tony Gatlif's trilogy of the Romany people. The film is a visual depiction and historical record of Romany life in European and Middle Eastern countries. Even though the scenes are mostly planned, rehearsed, and staged there is not a conventional story line and the dialog does not explain activities from scene to scene. Instead, the film allows the viewer to have sometimes a glimpse, sometimes a more in-depth view of these people during different eras and in different countries, ranging from India, Egypt, Romania, Hungary, Slovakia, France, and Spain.<br /><br />The importance of music in Romany culture is clearly expressed throughout the film. It is a vital part of every event and an important means of communication. Everything they do is expressed with music. Dance is another important activity. Like Romany music, it is specialized and deeply personal, something they alone know how to do correctly. We are provided glimpses into their everyday activities, but the film is not a detailed study of their lives. Rather, it is a testament to their culture, focusing on the music and dance they have created and which have made them unique.<br /><br />Mr. Gatlif portrays the nomadic groups in a positive way. However, we also witness the rejection, distrust, and alienation they receive from the non-Romany population. It seems that the culture they have developed over countless generations, and inspired from diverse countries, will fade into oblivion because conventional society has no place for nomadic ways.<br /><br />The other films in the trilogy are Les Princes (1983) and Gadjo Dilo (1998).\")]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.select('reviewpolarity', \"reviewrating\", \"featuresidf\", 'text').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model using N-fold stratified cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = list(set(stopwords.words('english')))\n",
    "\n",
    "stopwords_set[1:20]\n",
    "# stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Create a test df.\n",
    "df0 = transform_html_clean(df_pqt, 'textclean')\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_test\", stopWords=stopwords_set)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover])\n",
    "\n",
    "len(pipeline.fit(df0).transform(df0).head().words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the df into train and test\n",
    "df_training, df_test = df_pqt.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "df_training.count(), df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training.groupBy('reviewpolarity').count().show()\n",
    "df_test.groupBy('reviewpolarity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_training.drop('words')\n",
    "df_training = df_training.drop('featurestf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Based on Spark doc\n",
    "# https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)\n",
    "\n",
    "# The idea is to create a features vector from a list of words.\n",
    "\n",
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Or 2) use the Term Frequency - Inverse Document Frequency.\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, cv, idf, lr])\n",
    "\n",
    "# Define the criteria ranges.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100, 50000, 200000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Define the cross validation runner.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "df_training_tmp = df_training.withColumnRenamed('reviewpolarity', 'label')\n",
    "df_training_ppl = transform_html_clean(df_training_tmp, 'textclean')\n",
    "\n",
    "# Train the model.\n",
    "cvModel = crossval.fit(df_training_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_pip = model_best.transform(df_training_ppl)\n",
    "eval_val = evaluator.evaluate(df_training_pip)\n",
    "print(evaluator.isLargerBetter())\n",
    "print(eval_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training_pip.filter(df_training_pip.label == df_training_pip.prediction) \\\n",
    "    .select('label', 'probability', 'prediction', 'words').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the cross validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop('featurestf')\n",
    "df_test = df_test.drop('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data.\n",
    "df_test_tmp = df_test.withColumnRenamed('reviewpolarity','label')\n",
    "df_test_ppl = transform_html_clean(df_test_tmp, 'textclean')\n",
    "\n",
    "# Make prediction.\n",
    "df_test_res = model_best.transform(df_test_ppl)\n",
    "df_test_res.select('probability', 'label','prediction', 'features', 'words').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluator.evaluate(df_test_res))\n",
    "# df_test_res.filter(df_test_res.label == df_test_res.prediction) \\\n",
    "#     .select('label', 'probability', 'prediction', 'features', 'words').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd_training_pip = df_training_pip.select('prediction', 'label').rdd.map(lambda row: (row[0], float(row[1])))\n",
    "rdd_training_pip.take(2)\n",
    "\n",
    "# print(rdd_training_pip.toDF().toPandas().shape)\n",
    "\n",
    "metrics = MulticlassMetrics(rdd_training_pip)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print()\n",
    "print(metrics.truePositiveRate(1.0))\n",
    "print(metrics.falsePositiveRate(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiver Operating Characteristics (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionSummary\n",
    "\n",
    "# Get the Logistic regression model to get the summary.\n",
    "summary = cvModel.bestModel.stages[-1].summary\n",
    "summary.roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# As defined by IPython matplotlib kernel\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/plotting.html#id1\n",
    "%matplotlib inline\n",
    "\n",
    "aPlt = summary.roc.toPandas().plot(x='FPR', y='TPR', colormap='winter_r')\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], linestyle='--', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent for online and out-of-core learning Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the df_csv loaded earlier.\n",
    "print(\"%s entries from the CSV file\" % df_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator to load the data from the file simulating a streaming.\n",
    "ttl = 100000\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "def stream_doc():\n",
    "    with open(file_csv, 'r', encoding='utf-8') as csv:\n",
    "        # skip header.\n",
    "        next(csv)\n",
    "        \n",
    "        for line in csv:\n",
    "            cells = line.split(',')\n",
    "#             datasettype,filename,datetimecreated,reviewid,reviewpolarity,reviewrating,text = cells[0], \\\n",
    "#             cells[1], cells[2], cells[3], cells[4], cells[5], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            filename,reviewpolarity,text = cells[1], cells[4], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            yield filename,reviewpolarity,text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = stream_doc()\n",
    "print(next(generator))\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a number of documents (id, text) and their label from the doc stream.\n",
    "def get_mini_batch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            filename,reviewpolarity,text = next(doc_stream)\n",
    "            docs.append([filename, text])\n",
    "            y.append(int(reviewpolarity))\n",
    "    except StopIteration:\n",
    "        return docs, y\n",
    "    \n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the function we just wrote.\n",
    "get_mini_batch(stream_doc(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of SciKit Learn data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "# from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "boston = load_boston()\n",
    "print(type(boston.data[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)\n",
    "\n",
    "# The idea is to create a features vector from a list of words.\n",
    "\n",
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF])\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5, shuffle=True)\n",
    "\n",
    "# Get the X and y labels.\n",
    "def generate_X_y_labels(size):\n",
    "    data_batch, labels_batch = get_mini_batch(data_stream, size)\n",
    "    \n",
    "    if not data_batch: return np.empty(), np.empty()\n",
    "    \n",
    "    df_batch = spark.createDataFrame(data_batch, ('id', 'text'))\n",
    "\n",
    "    # Data cleansing.\n",
    "    df_batch_clean = transform_html_clean(df_batch, 'textclean')\n",
    "    df_training_tmp = df_batch_clean.withColumnRenamed('reviewpolarity', 'label')\n",
    "\n",
    "    # Run the tokenizer and remover pipeline.\n",
    "    m_pip = pipeline.fit(df_training_tmp)\n",
    "    df_pip_batch = m_pip.transform(df_training_tmp)\n",
    "    # Update the SGD regression weights.\n",
    "\n",
    "    # Let's get the right shape for the SparseVector data into numpy arrays.\n",
    "    series = df_pip_batch.toPandas()['features'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)\n",
    "    X = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "    y_labels =  np.array(labels_batch)\n",
    "\n",
    "    return X, y_labels\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# print(X[:])\n",
    "# print(y_labels[1:10])\n",
    "\n",
    "# Simulating a streaming\n",
    "data_stream = stream_doc()\n",
    "\n",
    "# Train the 45000 data from the entire data set.\n",
    "for i in range(4):\n",
    "    print(\"range %i\" % i)\n",
    "    X_train, y_labels_train = generate_X_y_labels(1000)\n",
    "    if not len(X_train): break\n",
    "        \n",
    "    model_sgd = clf.partial_fit(X_train, y_labels_train, classes=classes)\n",
    "\n",
    "# Test on the last 5000 entries.\n",
    "X_test, y_labels_test = generate_X_y_labels(5000)\n",
    "\n",
    "print(X_test)\n",
    "if len(X_test):\n",
    "    print(\"\\nscore: %.3f\" % model_sgd.score(X_test, y_labels_test))\n",
    "else:\n",
    "    print('No data')\n",
    "\n",
    "# Train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark V2.3.2 (Local)",
   "language": "python",
   "name": "pyspark-2.3.2-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
