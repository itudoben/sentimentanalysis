{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This notbook keeps all the examples used to learn how to use a particular technology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis of a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sample text'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "\n",
    "_remove_noise(\"this is a sample text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base path for the ACL IMDB data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the reviews into data frames.\n",
    "base_path = \"/Users/hujol/Projects/advanced_analytics_spark/data/aclImdb/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the files into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Python system path to find our modules.\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our modules.\n",
    "import file_loader as fl\n",
    "\n",
    "# Add the file to SparkContext for the executor to find it.\n",
    "sc.addPyFile('../src/file_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in a parquet file.\n",
    "file_pqt, df = fl.load_data(base_path, 252, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README                     aclImdb_250.csv\n",
      "aclImdb_100000.csv         \u001b[34maclImdb_250_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_100000_raw.parquet\u001b[m\u001b[m \u001b[34maclImdb_251_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_10000_raw.parquet\u001b[m\u001b[m  \u001b[34maclImdb_252_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_1000_raw.parquet\u001b[m\u001b[m   \u001b[34maclImdb_300_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_100_raw.parquet\u001b[m\u001b[m    \u001b[34maclImdb_301_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_20000_raw.parquet\u001b[m\u001b[m  \u001b[34maclImdb_50000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_2000_raw.parquet\u001b[m\u001b[m   imdb.vocab\n",
      "\u001b[34maclImdb_200_raw.parquet\u001b[m\u001b[m    imdbEr.txt\n",
      "\u001b[34maclImdb_210_raw.parquet\u001b[m\u001b[m    \u001b[34mtest\u001b[m\u001b[m\n",
      "\u001b[34maclImdb_211_raw.parquet\u001b[m\u001b[m    \u001b[34mtrain\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls {base_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on few sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(text=\"hello <br /> I'm [ doing this ] hopefully that works great!\", no_html=\"hello  I'm  doing this  hopefully that works great!\")]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_rdd = sc.parallelize([(\"hello <br /> I'm [ doing this ] hopefully that works great!\",)])\n",
    "df = a_rdd.toDF(['text'])\n",
    "\n",
    "\n",
    "# rdd = sc.parallelize([(1,2,3),(4,5,6),(7,8,9)])\n",
    "# df = rdd.toDF([\"a\",\"b\",\"c\"])\n",
    "df\n",
    "\n",
    "remover = fl.HTMLTagsRemover(inputCol='text', outputCol='no_html')\n",
    "remover.transform(df).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store as CVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ttl = 250\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "# Get the data as Pandas data frame.\n",
    "pdf = df.toPandas()\n",
    "\n",
    "# Re index to shuffle the data before saving it.\n",
    "pdf = pdf.reindex(np.random.permutation(pdf.index))\n",
    "pdf.to_csv(file_csv, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the CSV file for checking data is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datasettype</th>\n",
       "      <th>filename</th>\n",
       "      <th>datetimecreated</th>\n",
       "      <th>reviewid</th>\n",
       "      <th>reviewpolarity</th>\n",
       "      <th>reviewrating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test</td>\n",
       "      <td>9487_1.txt</td>\n",
       "      <td>20181025T084215Z</td>\n",
       "      <td>9487</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I have seen this movie and I did not care for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test</td>\n",
       "      <td>4604_4.txt</td>\n",
       "      <td>20181025T084215Z</td>\n",
       "      <td>4604</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>In Los Angeles, the alcoholic and lazy Hank Ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  datasettype    filename   datetimecreated  reviewid  reviewpolarity  \\\n",
       "1        test  9487_1.txt  20181025T084215Z      9487               0   \n",
       "2        test  4604_4.txt  20181025T084215Z      4604               0   \n",
       "\n",
       "   reviewrating                                               text  \n",
       "1             1  I have seen this movie and I did not care for ...  \n",
       "2             4  In Los Angeles, the alcoholic and lazy Hank Ch...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ttl = 250\n",
    "\n",
    "# Define the CSV file.\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "pdf_read = pd.read_csv(file_csv, encoding='utf-8')\n",
    "pdf_read[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "|datasettype|   filename| datetimecreated|reviewid|reviewpolarity|reviewrating|                text|\n",
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "|       test| 1821_4.txt|20181025T084215Z|    1821|             0|           4|Alan Rickman & Em...|\n",
      "|       test| 9487_1.txt|20181025T084215Z|    9487|             0|           1|I have seen this ...|\n",
      "|       test| 4604_4.txt|20181025T084215Z|    4604|             0|           4|In Los Angeles, t...|\n",
      "|       test| 2828_2.txt|20181025T084215Z|    2828|             0|           2|This film is bund...|\n",
      "|       test|10890_1.txt|20181025T084215Z|   10890|             0|           1|I only comment on...|\n",
      "|       test| 3351_4.txt|20181025T084215Z|    3351|             0|           4|When you look at ...|\n",
      "|       test| 8070_2.txt|20181025T084215Z|    8070|             0|           2|Rollerskating vam...|\n",
      "|       test| 1027_4.txt|20181025T084215Z|    1027|             0|           4|Technically abomi...|\n",
      "|       test| 8248_3.txt|20181025T084215Z|    8248|             0|           3|When Hollywood is...|\n",
      "|       test| 4290_4.txt|20181025T084215Z|    4290|             0|           4|Respected western...|\n",
      "|       test|10096_1.txt|20181025T084215Z|   10096|             0|           1|Worst movie ever ...|\n",
      "|       test|11890_1.txt|20181025T084215Z|   11890|             0|           1|I was forced to w...|\n",
      "|       test| 2008_1.txt|20181025T084215Z|    2008|             0|           1|Well it is about ...|\n",
      "|       test|  472_4.txt|20181025T084215Z|     472|             0|           4|Man with the Scre...|\n",
      "|       test| 9876_2.txt|20181025T084215Z|    9876|             0|           2|I never read the ...|\n",
      "|       test|11402_1.txt|20181025T084215Z|   11402|             0|           1|One of the movies...|\n",
      "|       test| 8487_1.txt|20181025T084215Z|    8487|             0|           1|Well I had the ch...|\n",
      "|       test| 6648_2.txt|20181025T084215Z|    6648|             0|           2|This is a movie t...|\n",
      "|       test| 2780_4.txt|20181025T084215Z|    2780|             0|           4|Dark Harvest is a...|\n",
      "|       test| 1789_2.txt|20181025T084215Z|    1789|             0|           2|A handful of nubi...|\n",
      "+-----------+-----------+----------------+--------+--------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv = spark.createDataFrame(pdf_read)\n",
    "df_csv.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data from the parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|datasettype|    filename| datetimecreated|reviewid|reviewpolarity|reviewrating|                text|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "|       test| 10813_7.txt|20181025T095457Z|   10813|             1|           7|The movie takes p...|\n",
      "|       test|11813_10.txt|20181025T095457Z|   11813|             1|          10|The Cure is a fan...|\n",
      "+-----------+------------+----------------+--------+--------------+------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the parquet file is good.\n",
    "df_pqt = spark.read.parquet(file_pqt)\n",
    "\n",
    "# As needed.\n",
    "# df_pqt = df_pqt.drop('words')\n",
    "\n",
    "# Showing some observations (entries).\n",
    "df_pqt.persist()\n",
    "df_pqt.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text to clean HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the secrets] of the universe. <br /><br />Unfortunately, \n",
      "the secrets of the universe. Unfortunately, \n"
     ]
    }
   ],
   "source": [
    "# Let's test our cleaning functions.\n",
    "a_text = 'the secrets] of the universe. <br /><br />Unfortunately, '\n",
    "print(a_text)\n",
    "print(fl.clean_html(a_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3471,\n",
       " 3377,\n",
       " 'Sophisticated sex comedies are always difficult to pull off. Look at the films of Blake Edwards, who is arguably the master of the genre, and you will find just as many misses as hits. For, if a film of this nature ever fails to work, it can never fall back on the tried and true toilet humor of a te',\n",
       " 'Sophisticated sex comedies are always difficult to pull off. Look at the films of Blake Edwards, who is arguably the master of the genre, and you will find just as many misses as hits. For, if a film of this nature ever fails to work, it can never fall back on the tried and true toilet humor of a te')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the parquet data frame.\n",
    "df_pqt = fl.transform_html_clean(df_pqt, 'textclean')\n",
    "tt = df_pqt.select('textclean', 'text').take(5)\n",
    "i = 4\n",
    "len(tt[i]['text']), len(tt[i]['textclean']), tt[i]['text'][:300], tt[i]['textclean'][:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write: write\n",
      "writer: writer\n",
      "writing: write\n",
      "writers: writer\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "words = ['write','writer','writing','writers']\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for word in words:\n",
    "    print(f\"{word}: {ps.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer.lemmatize('dogs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Features Extractor Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(words=['the', 'movie', 'takes', 'place', 'during', 'the', 'year', '1940', 'and', 'the', 'french', 'are', 'about', 'to', 'loose', 'the', 'war.the', 'movie', 'includes', 'all', 'genres:', 'comedy,', 'romantic,', 'murder', 'and', 'history.', 'it', 'is', 'probable', 'the', 'historical', 'part', 'may', 'be', 'not', 'as', 'probable', 'as', 'the', 'rest.it', 'is', 'not,', 'however,', 'a', 'big', 'laugh', 'movie', 'but', 'the', 'occasional', 'large', 'smile!'])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "df_pqt = df_pqt.drop('words')\n",
    "            \n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words\")\n",
    "df_pqt = tokenizer.transform(df_pqt)\n",
    "\n",
    "df_pqt.select('words').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['great', 'kitesurfing', 'long']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def row_text_cleaner(words):\n",
    "    words_clean = []\n",
    "    for a_w in words:\n",
    "        if not a_w in stopwords_bc.value:\n",
    "            words_clean.append(a_w)\n",
    "    \n",
    "    # Return the cleaned words.\n",
    "    return words_clean\n",
    "\n",
    "test_words = ['it', 'is', 'great', 'you', 'have', 'been', 'kitesurfing', 'that', 'long']\n",
    "row_text_cleaner(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          wordsclean|\n",
      "+--------------------+\n",
      "|[movie, takes, pl...|\n",
      "|[cure, fantastic,...|\n",
      "|[original, female...|\n",
      "|[remember, back, ...|\n",
      "|[sophisticated, s...|\n",
      "|[stumbled, upon, ...|\n",
      "|[film, tends, get...|\n",
      "|[well, done, acti...|\n",
      "|[strange, relatio...|\n",
      "|[brilliant,, lavi...|\n",
      "|[read, book, saw,...|\n",
      "|[played, sam, por...|\n",
      "|[\"snow, queen\", b...|\n",
      "|[watching, john, ...|\n",
      "|[luminously, phot...|\n",
      "|[@, 13, yrs, age,...|\n",
      "|[title:, opera, 1...|\n",
      "|['crossing, bridg...|\n",
      "|[gruelling, watch...|\n",
      "|[bought, first, z...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row \n",
    "\n",
    "def f(x):\n",
    "    data = x.asDict()\n",
    "    data['wordsclean'] = row_text_cleaner(x.words)\n",
    "    \n",
    "    # The purpose of ** is to give the ability to feed a function's arguments \n",
    "    # by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).\n",
    "    return Row(**data)\n",
    "\n",
    "# NOTE:\n",
    "# There is a need to store the result into df_pqt2 otherwise the\n",
    "# added words_clean added column does not show well if we store it in the same df_pqt when running:\n",
    "# df_pqt.select('words_clean').show()\n",
    "rdd_tmp = df_pqt.rdd.map(f)\n",
    "df_pqt = rdd_tmp.toDF()\n",
    "\n",
    "df_pqt.select('wordsclean').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- textclean: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "root\n",
      " |-- datasettype: string (nullable = true)\n",
      " |-- datetimecreated: string (nullable = true)\n",
      " |-- filename: string (nullable = true)\n",
      " |-- reviewid: long (nullable = true)\n",
      " |-- reviewpolarity: long (nullable = true)\n",
      " |-- reviewrating: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- words: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- wordsclean: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove intermediairy data not needed anymore.\n",
    "df_pqt.printSchema()\n",
    "# df_pqt = df_pqt.withColumnRenamed('words_clean', 'words')\n",
    "df_pqt = df_pqt.drop('textclean')\n",
    "df_pqt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computation of the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingTF testing on small vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|words                                                                                                             |rawFeatures                                                                                                                                 |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[hi, i, heard, about, spark, and, i, love, spark, with, java,, i, read, a, lot, about, java, and, spark,, sparky!]|(5096,[56,338,565,677,1568,1722,2321,2455,2799,2905,3001,3202,4672,4673,4959],[1.0,1.0,1.0,2.0,1.0,1.0,3.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0])|\n",
      "|[i, wish, java, could, use, case, classes]                                                                        |(5096,[449,1911,2085,2321,2799,4361,4390],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                                                                    |\n",
      "|[logistic, regression, models, are, neat]                                                                         |(5096,[220,682,1449,1502,2111],[1.0,1.0,1.0,1.0,1.0])                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "\n",
    "sentenceData = spark.createDataFrame([\n",
    "    (0, \"Hi I heard about Spark and I love SPark with Java, I read a lot about java and spark, sparky!\"),\n",
    "    (0, \"I wish Java could use case classes\"),\n",
    "    (1, \"Logistic regression models are neat\")\n",
    "], [\"label\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n",
    "wordsData = tokenizer.transform(sentenceData)\n",
    "\n",
    "# This uses the hash and the modulo numFeatures to define a bucket where to put a word.\n",
    "# It is efficient as it does not store the vocabulary.\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=5096)\n",
    "featurizedData = hashingTF.transform(wordsData)\n",
    "\n",
    "featurizedData.select('words', 'rawFeatures').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer test on small data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"words\", outputCol=\"rawfeatures\", vocabSize=70, minDF=1.0)\n",
    "model = cv.fit(featurizedData)\n",
    "result = model.transform(featurizedData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(rawfeatures=SparseVector(25, {0: 3.0, 1: 2.0, 2: 1.0, 3: 2.0, 4: 2.0, 8: 1.0, 10: 1.0, 11: 1.0, 13: 1.0, 14: 1.0, 16: 1.0, 17: 1.0, 20: 1.0, 21: 1.0, 24: 1.0}), words=['hi', 'i', 'heard', 'about', 'spark', 'and', 'i', 'love', 'spark', 'with', 'java,', 'i', 'read', 'a', 'lot', 'about', 'java', 'and', 'spark,', 'sparky!'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.select('rawfeatures', 'words').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "df_pqt = df_pqt.drop('featurestf')\n",
    "cv = CountVectorizer(inputCol=\"wordsclean\", outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "model_cv = cv.fit(df_pqt)\n",
    "df_pqt = model_cv.transform(df_pqt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in the corpus: 11221\n",
      "Excerpt of the vocabulary\n",
      "['film', 'like', 'one', 'really', '-', 'good', 'would', 'even', 'see', 'get', 'time', 'people', 'great', 'story', 'think', 'much', 'also', 'two', 'first', 'it.', 'watch', 'little', 'characters', 'never', 'could', 'bad', 'many', 'plot', 'love', 'make', 'made', 'way', 'movies', 'still', 'know', 'character', 'ever', 'well', 'films', 'seen', 'movie.', 'show', 'say', 'going', 'acting', \"i'm\", 'every', 'something', 'look', 'want', 'take', 'watching', 'best', 'real', 'better', 'film,', 'go', '&', 'nothing', 'back', 'man', 'may', 'actors', 'film.', 'movie,', 'things', 'long', 'scene', 'far', 'director', 'scenes', 'original', 'lot', 'another', 'horror', 'music', 'played', 'seems', 'part', 'old', 'gets', 'right', 'thought', 'us', 'funny', 'life', \"i've\", 'around', 'makes', 'almost', 'thing', 'yet', 'end', 'cast', 'though', 'it,', 'sound', 'however,', 'probably']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique words in the corpus: %s\" % len(model_cv.vocabulary))\n",
    "print(\"Excerpt of the vocabulary\\n\" + str(model_cv.vocabulary[1:100]))\n",
    "\n",
    "# result.select('features').rdd.map(lambda x: print(x)).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(11221, {0: 3.0, 62: 1.0, 79: 1.0, 98: 1.0, 117: 1.0, 142: 1.0, 227: 1.0, 354: 1.0, 435: 1.0, 480: 1.0, 829: 1.0, 984: 1.0, 1049: 1.0, 1160: 1.0, 1610: 1.0, 1942: 1.0, 2056: 1.0, 2204: 2.0, 4351: 1.0, 5252: 1.0, 5440: 1.0, 6686: 1.0, 7349: 1.0, 7812: 1.0, 9590: 1.0, 10443: 1.0})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.take(1)[0].featurestf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[datasettype: string, datetimecreated: string, filename: string, reviewid: bigint, reviewpolarity: bigint, reviewrating: bigint, text: string, words: array<string>, wordsclean: array<string>, featurestf: vector, featuresidf: vector]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import IDF\n",
    "\n",
    "# Drop the column first.\n",
    "df_pqt = df_pqt.drop('featuresidf')\n",
    "\n",
    "# IDF uses a term frequency vector:\n",
    "# http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html?highlight=tfidf#pyspark.mllib.feature.IDF\n",
    "idf = IDF(inputCol=\"featurestf\", outputCol=\"featuresidf\")\n",
    "idfModel = idf.fit(df_pqt)\n",
    "df_pqt = idfModel.transform(df_pqt)\n",
    "\n",
    "df_pqt.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(reviewpolarity=1, reviewrating=7, featuresidf=SparseVector(11221, {0: 1.9976, 62: 2.2012, 79: 2.1661, 98: 2.3553, 117: 2.3979, 142: 2.4889, 227: 2.8253, 354: 3.1355, 435: 3.2308, 480: 3.5875, 829: 3.5875, 984: 3.924, 1049: 3.924, 1160: 3.924, 1610: 4.1471, 1942: 4.1471, 2056: 4.1471, 2204: 9.6805, 4351: 4.8402, 5252: 4.8402, 5440: 4.8402, 6686: 4.8402, 7349: 4.8402, 7812: 4.8402, 9590: 4.8402, 10443: 4.8402}), text='The movie takes place during the year 1940 and the French are about to loose the war.<br /><br />The movie includes all genres: comedy, romantic, murder and history. It is probable the historical part may be not as probable as the rest.<br /><br />It is not, however, a big laugh movie but the occasional large smile!')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pqt.select('reviewpolarity', \"reviewrating\", \"featuresidf\", 'text').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a logistic regression for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model using N-fold stratified cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['me',\n",
       " 'up',\n",
       " 'myself',\n",
       " 'same',\n",
       " 'this',\n",
       " 'through',\n",
       " 'once',\n",
       " \"should've\",\n",
       " \"you've\",\n",
       " 'ours',\n",
       " 'both',\n",
       " 'wouldn',\n",
       " 'into',\n",
       " 'down',\n",
       " 'than',\n",
       " 'wasn',\n",
       " 've',\n",
       " 'hers',\n",
       " 'on']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = list(set(stopwords.words('english')))\n",
    "\n",
    "stopwords_set[1:20]\n",
    "# stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Create a test df.\n",
    "html_tag_remover = fl.HTMLTagsRemover(inputCol='text', outputCol='textclean')\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_test\", stopWords=stopwords_set)\n",
    "pipeline = Pipeline(stages=[html_tag_remover, tokenizer, remover])\n",
    "\n",
    "len(pipeline.fit(df_pqt).transform(df_pqt).head().words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 22)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the df into train and test\n",
    "df_training, df_test = df_pqt.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "df_training.count(), df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|reviewpolarity|count|\n",
      "+--------------+-----+\n",
      "|             0|  114|\n",
      "|             1|  116|\n",
      "+--------------+-----+\n",
      "\n",
      "+--------------+-----+\n",
      "|reviewpolarity|count|\n",
      "+--------------+-----+\n",
      "|             0|   13|\n",
      "|             1|    9|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_training.groupBy('reviewpolarity').count().show()\n",
    "df_test.groupBy('reviewpolarity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = df_training.drop('words')\n",
    "df_training = df_training.drop('featurestf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Based on Spark doc\n",
    "# https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step: create a features vector from a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HashingTF' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-21880e929084>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 1) Use this hashing Term Frequency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhashingTF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHashingTF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"features\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Or 2) use the Term Frequency - Inverse Document Frequency.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremover\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"featurestf\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminDF\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HashingTF' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Or 2) use the Term Frequency - Inverse Document Frequency.\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10)\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[html_tag_remover, tokenizer, remover, cv, idf, lr])\n",
    "\n",
    "# Define the criteria ranges.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [100, 50000, 200000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "# Define the cross validation runner.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "df_training_tmp = df_training.withColumnRenamed('reviewpolarity', 'label')\n",
    "df_training_ppl = fl.transform_html_clean(df_training_tmp, 'textclean')\n",
    "\n",
    "# Train the model.\n",
    "cvModel = crossval.fit(df_training_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7235029850323967,\n",
       " 0.7204781993605522,\n",
       " 0.7235029850323969,\n",
       " 0.7204781993605522,\n",
       " 0.7235029850323967,\n",
       " 0.7204781993605522]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.avgMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "df_training_pip = model_best.transform(df_training_ppl)\n",
    "eval_val = evaluator.evaluate(df_training_pip)\n",
    "print(evaluator.isLargerBetter())\n",
    "print(eval_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1, probability=DenseVector([0.001, 0.999]), prediction=1.0, words=['film', 'tends', 'get', 'buried', 'prejudice', 'preconception', '-', 'remake!', 'doris', 'day', 'it!', 'sings!', '-', \"hitchcock's\", 'second', 'crack', \"'the\", 'man', 'knew', \"much'\", 'under-rated', 'film,', 'arguably', 'fully', 'fledged', 'masterpiece', 'right.this', 'is,', 'ways', 'one,', 'doris', \"day's\", 'film.', 'give', 'finest', 'performance', 'career,', 'holding', 'james', 'stewart,', 'whole', 'film', 'subtly', 'structured', 'around', 'character', 'rather', 'his.', 'is,', 'all,', 'film', 'music', 'motif', 'plot', 'device.', 'better', 'casting', 'popular', 'singer', 'generation?', 'consider:', \"day's\", 'jo', 'mckenna', 'given', 'career', 'stage', 'order', 'settle', 'husband', 'raise', 'son.', 'seems', 'mutual', 'decision,', 'appear', 'unhappy.', 'look', 'way', 'stewart', 'teases', 'horse-drawn', 'carriage', 'concerns', 'louis', 'bernard,', 'implying', 'jealous', 'bernard', 'asking', 'questions', 'career.', 'clearly', 'recurrent', 'joke', '-', 'responds', \"'har-de-har-har'\", 'denotes', 'familiarity', 'gag,', 'suggesting', 'certain', 'latent', 'resentment', 'confinement,', 'realise', 'it.after', 'son', 'kidnapped,', 'stewart', 'insists', 'doping', 'giving', 'news.', 'cruel', 'scene,', 'brilliantly', 'played', 'actors,', 'illustrates', 'power', 'imbalance', 'marriage', '-', 'seeking', 'control', 'subdue', 'reactions,', 'essence', 'using', 'professional', 'knowledge', 'suppress', 'voice', 'marriage', 'medical', 'career', 'suppressed', 'singing', 'career.the', 'potency', 'voice', 'demonstrated', 'ambrose', 'chapel', 'sequence,', 'reign', 'highly', 'trained', 'clarity', 'volume', 'blend', 'congregation', 'female', 'drudges', '-', 'almost', 'act', 'warning', 'become', 'continues', 'suppress', 'talent.', 'albert', 'hall,', 'need', 'cry', 'out,', 'exercise', 'impressive', 'lungs,', 'saves', \"man's\", 'life,', 'embassy', 'finale,', 'talent', 'reputation', 'allows', 'locate', 'son.', 'contrast,', \"stewart's\", 'masculine', 'activity', 'counterproductive', '-', 'visit', 'taxidermist', 'dead', 'end,', 'gets', 'left', 'behind', 'church', 'whilst', 'everyone', 'else', 'moves', 'albert', 'hall,', 'efforts', 'succeed', 'getting', 'assassin', 'killed,', 'thus', 'depriving', 'police', 'potentially', 'useful', 'information.', 'action', 'joined', \"wife's\", 'voice,', 'rescue', 'hank', 'embassy,', 'actually', 'succeeds', 'something', 'useful.far', 'forced', 'film', 'give', 'day', 'opportunity', 'sing,', \"'que\", 'sera', \"sera'\", 'acts', 'first', 'musical', 'device', 'film,', 'foreshadowing', 'nightmare', 'engulf', 'mckennas;', \"'the\", \"future's\", \"see'\", 'indeed.', 'also', 'neatly', 'prepares', 'way', 'finale,', 'close', 'bond', 'mother', 'son', 'share', 'music', 'allow', 'doris', 'save', 'day.the', 'famous', 'sequence', 'film', 'makes', 'music', 'central', 'feature', '-', 'build', 'assassination', 'attempt', 'albert', 'hall.', 'lengthy', 'wordless', 'sequence', 'may', 'single', 'extraordinary', 'thing', 'hitchcock', 'committed', 'film,', 'ultimate', 'expression', 'belief', 'films', 'stories', 'told', 'visually.', 'see', 'people', 'conduct', 'conversations', 'sequence,', 'never', 'hear', 'word', 'say.', 'need', '-', 'images', 'say', 'everything.', 'also', 'exquisite', 'suspense', 'sequence,', 'pieces', 'moving', 'slowly', 'place', 'music', 'builds.', 'editing', 'incredibly', 'tight,', 'matched', 'music', 'perfectly.', 'frame', 'place', '-', 'anything', 'relate', 'directly', 'assassination', 'giving', 'viewer', 'sense', 'environment,', 'geography', 'playing', 'out.', 'builds', 'slowly,', 'end', 'suspense', 'nearly', 'unbearable.', 'jo', 'screams,', 'relief', 'her,', 'audience.the', 'ambrose', 'chapel', 'sequence', 'witty,', 'particularly', 'effective', 'anyone', 'sit', 'service', 'particularly', 'stick-in-the-mud', 'nonconformist', 'church.', 'embassy', 'sequence', 'seems', 'little', 'flat', 'albert', 'hall', 'one', 'preceded', 'first', 'viewing,', 'second', 'time', 'around', 'actually', 'seems', 'effective,', 'final', 'walk', 'gunpoint', 'really', 'benefiting', 'gorgeous', 'use', 'day', 'singing', 'background,', 'reminiscent', 'music-as-ambient-noise', \"'rear\", \"window'.\", 'score', 'whole', 'subtle,', 'allowing', 'music', 'on-screen', 'sources', 'foregrounded', 'effectively.bernard', 'miles', 'low-key', 'villain,', 'little', 'banal,', 'dry', 'wit.', \"he's\", 'outshone', 'brenda', 'de', 'banzie', 'wife,', 'walks', 'fine', 'line', 'sinister', 'sympathetic.', 'look', 'way', 'smokes', 'cigarette', 'whilst', 'husband', 'preps', 'assassin', '-', 'stance', 'pure', \"gangster's\", 'moll,', 'belying', 'middle-england', 'exterior,', 'clearly', 'soft', 'side,', 'possibly', 'maternal', 'feelings', 'towards', 'hank.stewart', 'excellent,', 'although', 'hitchcock', 'really', 'always', 'cast', \"'everyman',\", \"director's\", 'daughter', 'seems', 'think,', 'confirms', 'hitchcock', 'cynical', 'view', 'audience.', 'stewart', 'played', 'hypocritical', 'intellectual', 'espoused', 'fascist', 'ideology', 'rope,', 'voyeur', 'mistreated', 'girlfriend', 'rear', 'window', 'obsessive', 'necrophiliac', 'vertigo.', 'day', 'nothing', 'short', 'phenomenal.', 'look', 'reaction', 'news', 'son', 'kidnapped', '-', 'never', 'overdoes', 'anything,', 'neither', 'sell', 'short.', 'one', \"hitchcock's\", 'emotionally', 'effective', 'films.', 'never', 'lets', 'us', 'forget', 'stakes', 'mckennas;', 'feel', 'fully', 'human', 'central', 'characters.']),\n",
       " Row(label=1, probability=DenseVector([0.0306, 0.9694]), prediction=1.0, words=['movie', 'takes', 'place', 'year', '1940', 'french', 'loose', 'war.the', 'movie', 'includes', 'genres:', 'comedy,', 'romantic,', 'murder', 'history.', 'probable', 'historical', 'part', 'may', 'probable', 'rest.it', 'not,', 'however,', 'big', 'laugh', 'movie', 'occasional', 'large', 'smile!'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_training_pip.filter(df_training_pip.label == df_training_pip.prediction) \\\n",
    "    .select('label', 'probability', 'prediction', 'words').take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the cross validation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop('featurestf')\n",
    "df_test = df_test.drop('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on using the HTMLTagRemover Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(probability=DenseVector([0.0533, 0.9467]), label=1, prediction=1.0, features=SparseVector(10485, {1: 2.1987, 3: 2.6656, 5: 11.6076, 7: 1.0604, 8: 2.2234, 16: 1.4351, 17: 1.4351, 18: 4.9747, 21: 1.4721, 25: 1.8315, 26: 1.5923, 28: 1.8048, 30: 1.6582, 31: 1.5923, 39: 1.8048, 42: 2.1466, 45: 1.8315, 52: 3.663, 53: 1.8589, 54: 1.8589, 59: 12.5593, 60: 1.8871, 63: 2.1466, 64: 2.0084, 65: 4.4471, 66: 2.1466, 68: 2.1466, 71: 2.1466, 73: 2.1843, 74: 2.1843, 82: 2.3514, 83: 2.2644, 85: 2.1466, 86: 2.2644, 94: 4.4471, 101: 2.3979, 120: 5.1041, 124: 2.3514, 128: 4.5287, 146: 4.996, 149: 2.498, 151: 2.552, 167: 4.996, 173: 2.7344, 184: 2.8034, 194: 2.6698, 196: 2.6698, 208: 2.8775, 209: 2.6092, 212: 2.6092, 223: 8.6324, 225: 2.9575, 242: 2.8034, 290: 2.8775, 298: 3.0445, 316: 5.7549, 328: 3.1398, 331: 5.915, 332: 3.0445, 344: 2.9575, 367: 2.9575, 369: 2.9575, 371: 2.9575, 381: 3.1398, 391: 3.363, 396: 3.1398, 400: 3.2452, 414: 3.1398, 430: 6.4904, 440: 10.0889, 453: 3.1398, 456: 3.2452, 470: 3.2452, 476: 3.1398, 491: 3.363, 502: 3.2452, 513: 6.4904, 559: 6.4904, 568: 3.363, 569: 3.4965, 589: 3.4965, 595: 3.363, 605: 3.363, 625: 3.363, 628: 3.4965, 654: 3.4965, 658: 3.4965, 667: 3.4965, 708: 3.6507, 751: 3.6507, 755: 3.6507, 759: 3.4965, 763: 3.833, 766: 3.6507, 794: 3.4965, 813: 3.6507, 846: 3.6507, 847: 3.6507, 880: 3.6507, 894: 3.6507, 899: 3.6507, 902: 3.6507, 939: 3.833, 940: 3.6507, 957: 3.6507, 964: 10.952, 1001: 3.6507, 1015: 3.833, 1031: 3.833, 1037: 3.833, 1102: 3.833, 1150: 12.1684, 1169: 3.833, 1172: 3.833, 1190: 3.833, 1222: 7.666, 1256: 3.833, 1341: 4.3438, 1355: 4.0561, 1382: 4.0561, 1400: 4.0561, 1469: 4.0561, 1515: 4.0561, 1593: 4.0561, 1594: 4.0561, 1616: 4.0561, 1630: 4.0561, 1635: 4.0561, 1641: 4.0561, 1656: 4.0561, 1665: 4.3438, 1688: 4.0561, 1825: 4.0561, 1922: 4.0561, 1974: 4.3438, 2058: 4.3438, 2171: 4.3438, 2267: 4.3438, 2294: 4.3438, 2369: 8.6876, 2371: 4.3438, 2414: 4.3438, 2712: 4.3438, 2741: 4.3438, 2753: 4.3438, 2770: 4.3438, 2938: 4.3438, 2941: 4.3438, 2993: 4.3438, 3116: 4.3438, 3504: 4.7493, 3566: 4.7493, 3642: 4.7493, 3695: 4.7493, 3830: 4.7493, 3935: 4.7493, 4287: 4.7493, 4920: 4.7493, 5092: 4.7493, 5254: 4.7493, 5484: 4.7493, 5512: 4.7493, 5806: 4.7493, 5902: 4.7493, 5965: 4.7493, 6039: 4.7493, 6161: 4.7493, 6456: 4.7493, 6477: 4.7493, 6480: 4.7493, 6660: 4.7493, 6937: 4.7493, 7091: 4.7493, 7152: 4.7493, 7344: 4.7493, 7456: 9.4985, 7503: 4.7493, 7547: 4.7493, 7762: 4.7493, 8077: 4.7493, 8113: 4.7493, 8343: 4.7493, 8425: 4.7493, 8449: 4.7493, 8457: 4.7493, 8643: 4.7493, 8972: 4.7493, 9088: 4.7493, 9289: 4.7493, 9570: 4.7493, 9800: 4.7493, 10075: 4.7493, 10378: 4.7493}), words=['sophisticated', 'sex', 'comedies', 'always', 'difficult', 'pull', 'off.', 'look', 'films', 'blake', 'edwards,', 'arguably', 'master', 'genre,', 'find', 'many', 'misses', 'hits.', 'for,', 'film', 'nature', 'ever', 'fails', 'work,', 'never', 'fall', 'back', 'tried', 'true', 'toilet', 'humor', 'teen', 'sex', 'comedy', 'i.e.', '\"american', 'pie\",', 'warm', 'audience', 'sentimentality', 'romantic', 'comedy', 'i.e.', 'julia', \"roberts'\", 'entire', 'career.', 'maintain', 'push', 'end,', 'hope', 'audience', 'appreciate', 'almost', 'required', 'irony', 'resolution.written', 'husband/wife', 'team', 'wally', 'wolodarsky', 'maya', 'forbes,', '\"seeing', 'people\"', 'opens', 'engaged', 'couple', 'ed', '&', 'alice', 'jay', 'mohr', '&', 'julianne', 'nicholson', 'seconds', 'away', 'rear-ending', 'car', 'front', 'them.', 'frame', 'freezes,', 'unexpectedly', 'hear', 'thoughts', 'fears', 'characters.', 'out,', 'welcome', 'story', 'unfold', 'enjoy', 'point', 'view', 'sexes.two', 'months', 'shy', 'vows,', 'ed', '&', 'alice', 'already', 'look', 'act', 'like', 'old', 'married', 'couple.', 'early', 'bathroom', 'scene,', 'actions', 'alone', 'show', 'us', 'comfortable', 'long', 'together.', 'line', 'propel', 'plot', 'forward', 'uttered', '-', 'expectedly', 'least', 'likely', 'two', '-', 'relationship', 'calling', 'change,', 'even', 'means', 'destruction.once', 'ground', 'rules', 'set', 'ed', 'sleep', 'mother', 'or,', 'matter,', 'salma', 'hayek,', 'two', 'head', 'separate', 'directions', 'hope', 'finding', 'meaningless', 'sex', 'strengthen', 'relationship.', 'first,', 'everything', 'seems', 'go', 'planned', 'daily', 'trysts', 'help', 'fire', 'passion', 'them.', 'predictably,', 'deeper', 'emotions', 'regret', 'jealousy', 'begin', 'emerge,', 'soon', 'find', 'growing', 'apart', 'verge', 'breaking', 'up.', 'actions', 'leading', 'resolution', 'may', 'may', 'like', '-', 'depending', 'degree', 'cynicism.for', 'comedy', 'like', 'this,', 'need', 'solid', 'cast', 'supporting', 'characters', 'strong', 'leads.', 'director', 'wolodarsky', 'disappoint.', 'cast', 'two', 'favorite', 'actresses', 'sisters', '-', 'julianne', 'nicholson', '&', 'lauren', 'graham', '-', 'allows', 'play', 'strengths.', 'nicholson,', 'always', 'reminded', 'young', 'shirley', 'maclaine,', 'brings', 'air', 'naivete', 'vulnerability', 'alice', 'even', 'actions', 'seems', 'less', 'so.', 'graham,', 'actress', 'proven', 'could', 'outperform', 'entire', 'howard', 'hawks', 'ensemble,', 'steals', 'every', 'scene', 'edgy', '\"no', 'bs\"', 'persona.', 'guys,', 'jay', 'mohr', 'serviceable', 'josh', 'charles.', '\"malcolm', 'middle\"\\'s', 'byron', 'cranston', 'applauded', 'taking', 'british', 'accent', 'letting', 'hang', 'out.', 'real', 'treat', 'andy', 'richter', 'sub-plot', 'involving', 'single', 'mother,', 'helen', 'slater.', 'scenes', 'almost', 'seem', 'belong', 'another', 'movie,', 'far', 'funniest', 'dead', 'panned', 'delivery', 'steals', 'show.for', 'independent', 'production,', '\"seeing', 'people\"', 'personal', 'introspective', 'feeling', '-', 'something', 'would', 'noticeable', 'absent', 'big', 'hollywood', 'film', 'kind.', 'mention', 'film', 'also', 'genuinely', 'funny', 'moments', '-', 'unlike,', 'say,', 'hollywood', 'comedies', 'general.rating', '5', 'star', 'system', ':', '3', '1/2', 'stars']),\n",
       " Row(probability=DenseVector([0.1803, 0.8197]), label=1, prediction=1.0, features=SparseVector(10485, {1: 1.4658, 2: 0.6975, 3: 0.8885, 6: 2.2499, 7: 1.0604, 8: 1.1117, 12: 2.6961, 13: 1.3315, 18: 1.6582, 28: 1.8048, 29: 1.8048, 46: 1.8315, 51: 3.7178, 57: 1.9459, 62: 4.2932, 68: 2.1466, 71: 2.1466, 74: 4.3686, 75: 2.3979, 85: 4.2932, 87: 4.5287, 89: 2.3069, 98: 2.3069, 124: 2.3514, 126: 2.498, 134: 2.3514, 144: 2.498, 146: 2.498, 166: 2.552, 183: 2.6092, 184: 2.8034, 187: 2.6092, 193: 2.6698, 194: 2.6698, 197: 2.7344, 198: 2.7344, 210: 2.6698, 212: 2.6092, 214: 2.8775, 215: 2.8034, 296: 2.8775, 348: 3.0445, 359: 3.0445, 397: 3.1398, 399: 3.0445, 446: 6.726, 449: 3.6507, 482: 10.0889, 537: 3.2452, 558: 3.363, 562: 7.3013, 573: 3.363, 601: 3.363, 605: 3.363, 629: 3.363, 643: 3.6507, 667: 3.4965, 720: 3.6507, 774: 3.6507, 776: 3.6507, 794: 3.4965, 797: 4.0561, 824: 3.6507, 868: 4.3438, 880: 3.6507, 922: 3.6507, 959: 3.833, 1060: 3.833, 1116: 4.0561, 1142: 3.833, 1143: 7.666, 1205: 4.0561, 1213: 3.833, 1280: 3.833, 1311: 3.833, 1550: 4.3438, 1594: 4.0561, 1642: 4.0561, 1714: 4.0561, 1734: 4.3438, 1742: 4.0561, 1751: 4.3438, 1827: 4.0561, 1874: 4.0561, 1885: 4.0561, 1930: 4.3438, 1932: 12.1684, 1948: 4.0561, 2035: 4.3438, 2048: 4.3438, 2255: 4.3438, 2546: 4.3438, 2807: 4.3438, 2837: 4.7493, 2894: 4.3438, 2947: 4.3438, 3132: 4.3438, 3293: 4.3438, 3468: 4.7493, 4099: 4.7493, 4101: 4.7493, 4438: 4.7493, 4570: 4.7493, 5259: 4.7493, 5517: 4.7493, 5613: 4.7493, 5898: 4.7493, 6083: 4.7493, 6206: 4.7493, 6780: 4.7493, 7243: 4.7493, 7581: 9.4985, 7915: 4.7493, 8203: 4.7493, 8231: 4.7493, 8761: 4.7493, 9634: 4.7493, 9737: 4.7493, 9941: 4.7493, 10032: 4.7493, 10214: 4.7493}), words=['luminously', 'photographed', 'unusually', 'well-written', 'western', 'veteran', 'creator', '\"rawhide\"', 'charles', 'marquis', 'warren.', 'direcxtor', 'gordon', 'douglas', 'chief', 'help', 'regard.', 'strong', 'plot', 'line', 'told', 'sentences.', 'hard-nosed', 'by-the-', 'book,', 'cavalry', 'officer,', 'captain', 'richard', 'lance,', 'captures', 'leader', 'indian', 'enemy', 'massacre', 'fort.', 'insists', 'bringing', 'man', 'back', 'trial,', 'sent', 'totucson;', 'commander', 'sends', 'another', 'man', 'try', 'take', 'prisoner', 'trial', 'patrol', 'wiped', 'out.', 'means', 'leader', 'escaped,', 'lance', 'must', 'lead', 'second', 'patrol--and', 'picks', 'men', 'fort', 'spare,', 'company', 'problems--', 'defend', 'advance', 'fort', 'wiped', 'save', 'command', 'another', 'attack', 'stopping', 'bottleneck', 'pass', 'sector.', 'lance,', 'young', 'gregory', 'peck', 'quite', 'strong.', 'large', 'cast', 'film', 'really', 'shows', 'life', 'cavalry', 'outpost', 'looking', 'like', 'army', 'establishment', 'heterogeneous', 'quarreling', 'types', 'includes', 'war', 'bond', 'powerful', 'hard-drinking', 'sergeant,', 'neville', 'brand', 'steve', 'brodie', 'troublemakers,', 'warner', 'anderson', 'lon', 'chaney', 'jr.', 'psychological', 'troublemakers', 'gig', 'young,', 'art', 'baker,', 'herbert', 'heyes', 'fellow', 'officers', 'nana', 'bryant', \"colonel's\", 'wife.', 'even', 'barbara', 'payton', 'love', 'interest', 'gets', 'difficult', 'role;', 'michael', 'ansara', 'captured', 'war', 'chief,', 'jeff', 'corey', 'plays', \"fort's\", 'scout.', 'really', 'two', 'great', 'scenes', 'very-well-made', 'western--the', 'long', 'section', 'fort', 'last', 'patrol', 'sent', 'out,', 'long', 'patrol', 'doomed', 'ft.', 'defiant', 'itself.', 'fort,', 'peck', 'gets', 'deliver', 'grand', 'speech', 'demand', 'men', 'lined', 'orders,', 'tells', 'took', 'along.', 'reading', 'shortcomings', 'one', 'one;', 'tell', 'think', 'sent', 'best', 'friend', 'die', 'place', 'take', 'indian', 'instead', 'going', 'himself--', 'proves', 'wrong', 'remainder', 'film', 'winning', 'lonely', 'battle', 'intelligence', 'courage.', 'music', 'franz', 'waxman', 'good,', 'production', 'qualities', 'admirable;', 'argument', 'would', 'happen', 'lance', 'takes', 'war', 'chief', 'happens', 'true;', 'unsolvable', 'mistake', 'central', 'character,', 'great', 'western.', 'favorite', 'mine', 'fifty', 'years.'])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data.\n",
    "df_test_tmp = df_test.withColumnRenamed('reviewpolarity','label')\n",
    "df_test_ppl = fl.transform_html_clean(df_test_tmp, 'textclean')\n",
    "\n",
    "# Make prediction.\n",
    "df_test_res = model_best.transform(df_test_ppl)\n",
    "df_test_res.select('probability', 'label','prediction', 'features', 'words').take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8803418803418804\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(df_test_res))\n",
    "# df_test_res.filter(df_test_res.label == df_test_res.prediction) \\\n",
    "#     .select('label', 'probability', 'prediction', 'features', 'words').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114.   0.]\n",
      " [  0. 116.]]\n",
      "\n",
      "1.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "rdd_training_pip = df_training_pip.select('prediction', 'label').rdd.map(lambda row: (row[0], float(row[1])))\n",
    "rdd_training_pip.take(2)\n",
    "\n",
    "# print(rdd_training_pip.toDF().toPandas().shape)\n",
    "\n",
    "metrics = MulticlassMetrics(rdd_training_pip)\n",
    "print(metrics.confusionMatrix().toArray())\n",
    "print()\n",
    "print(metrics.truePositiveRate(1.0))\n",
    "print(metrics.falsePositiveRate(1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiver Operating Characteristics (ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression_42b6b369278ce1f2085b"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.bestModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017241379310344827|\n",
      "|0.0|0.034482758620689655|\n",
      "|0.0| 0.05172413793103448|\n",
      "|0.0| 0.06896551724137931|\n",
      "|0.0| 0.08620689655172414|\n",
      "|0.0| 0.10344827586206896|\n",
      "|0.0|  0.1206896551724138|\n",
      "|0.0| 0.13793103448275862|\n",
      "|0.0| 0.15517241379310345|\n",
      "|0.0|  0.1724137931034483|\n",
      "|0.0|  0.1896551724137931|\n",
      "|0.0| 0.20689655172413793|\n",
      "|0.0| 0.22413793103448276|\n",
      "|0.0|  0.2413793103448276|\n",
      "|0.0| 0.25862068965517243|\n",
      "|0.0| 0.27586206896551724|\n",
      "|0.0| 0.29310344827586204|\n",
      "|0.0|  0.3103448275862069|\n",
      "|0.0|  0.3275862068965517|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionSummary\n",
    "\n",
    "# Get the Logistic regression model to get the summary.\n",
    "summary = cvModel.bestModel.stages[-1].summary\n",
    "summary.roc.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# As defined by IPython matplotlib kernel\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/plotting.html#id1\n",
    "%matplotlib inline\n",
    "\n",
    "aPlt = summary.roc.toPandas().plot(x='FPR', y='TPR', colormap='winter_r')\n",
    "plt.plot([0.0, 1.0], [0.0, 1.0], linestyle='--', color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent for online and out-of-core learning Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252 entries from the CSV file\n"
     ]
    }
   ],
   "source": [
    "# Use the df_csv loaded earlier.\n",
    "print(\"%s entries from the CSV file\" % df_csv.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator to load the data from the file simulating a streaming.\n",
    "ttl = 100000\n",
    "file_csv = os.path.join(base_path, (\"aclImdb_%s.csv\" % ttl))\n",
    "\n",
    "def stream_doc():\n",
    "    with open(file_csv, 'r', encoding='utf-8') as csv:\n",
    "        # skip header.\n",
    "        next(csv)\n",
    "        \n",
    "        for line in csv:\n",
    "            cells = line.split(',')\n",
    "#             datasettype,filename,datetimecreated,reviewid,reviewpolarity,reviewrating,text = cells[0], \\\n",
    "#             cells[1], cells[2], cells[3], cells[4], cells[5], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            filename,reviewpolarity,text = cells[1], cells[4], \",\".join(cells[6:]).strip()\n",
    "\n",
    "            yield filename, reviewpolarity, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1821_4.txt', '0', '\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It\\'s worth seeing for their scenes- and Rickman\\'s scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\"')\n",
      "('9487_1.txt', '0', 'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.')\n"
     ]
    }
   ],
   "source": [
    "generator = stream_doc()\n",
    "print(next(generator))\n",
    "print(next(generator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function returns a number of documents (id, text) and their label from the doc stream.\n",
    "def get_mini_batch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            filename,reviewpolarity,text = next(doc_stream)\n",
    "            docs.append([filename, text])\n",
    "            y.append(int(reviewpolarity))\n",
    "    except StopIteration:\n",
    "        return docs, y\n",
    "    \n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['1821_4.txt',\n",
       "   '\"Alan Rickman & Emma Thompson give good performances with southern/New Orleans accents in this detective flick. It\\'s worth seeing for their scenes- and Rickman\\'s scene with Hal Holbrook. These three actors mannage to entertain us no matter what the movie, it seems. The plot for the movie shows potential, but one gets the impression in watching the film that it was not pulled off as well as it could have been. The fact that it is cluttered by a rather uninteresting subplot and mostly uninteresting kidnappers really muddles things. The movie is worth a view- if for nothing more than entertaining performances by Rickman, Thompson, and Holbrook.\"'],\n",
       "  ['9487_1.txt',\n",
       "   'I have seen this movie and I did not care for this movie anyhow. I would not think about going to Paris because I do not like this country and its national capital. I do not like to learn french anyhow because I do not understand their language. Why would I go to France when I rather go to Germany or the United Kingdom? Germany and the United Kingdom are the nations I tolerate. Apparently the Olsen Twins do not understand the French language just like me. Therefore I will not bother the France trip no matter what. I might as well stick to the United Kingdom and meet single women and play video games if there is a video arcade. That is all.']],\n",
       " [0, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the function we just wrote.\n",
    "get_mini_batch(stream_doc(), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of SciKit Learn data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "# from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "boston = load_boston()\n",
    "print(type(boston.data[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline.\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator , RegressionEvaluator\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, Tokenizer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "# Define the stages.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words\", stopWords=stopwords_set)\n",
    "\n",
    "# The idea is to create a features vector from a list of words.\n",
    "\n",
    "# 1) Use this hashing Term Frequency.\n",
    "hashingTF = HashingTF(inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, hashingTF])\n",
    "\n",
    "# The evaluator of each models.\n",
    "# evaluator = RegressionEvaluator(metricName=\"r2\")\n",
    "evaluator = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent Using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stream_doc' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-be3a11d57860>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Simulating a streaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdata_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'stream_doc' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "\n",
    "clf = SGDClassifier(loss=\"log\", penalty=\"l2\", max_iter=5, shuffle=True)\n",
    "\n",
    "# Get the X and y labels.\n",
    "def generate_X_y_labels(data_stream, size):\n",
    "    data_batch, labels_batch = get_mini_batch(data_stream, size)\n",
    "    \n",
    "    if not data_batch: return np.empty(), np.empty()\n",
    "    \n",
    "    df_batch = spark.createDataFrame(data_batch, ('id', 'text'))\n",
    "\n",
    "    # Data cleansing.\n",
    "    df_batch_clean = fl.transform_html_clean(df_batch, 'textclean')\n",
    "    df_training_tmp = df_batch_clean.withColumnRenamed('reviewpolarity', 'label')\n",
    "\n",
    "    # Run the tokenizer and remover pipeline.\n",
    "    m_pip = pipeline.fit(df_training_tmp)\n",
    "    df_pip_batch = m_pip.transform(df_training_tmp)\n",
    "    # Update the SGD regression weights.\n",
    "\n",
    "    # Let's get the right shape for the SparseVector data into numpy arrays.\n",
    "    series = df_pip_batch.toPandas()['features'].apply(lambda x : np.array(x.toArray())).as_matrix().reshape(-1,1)\n",
    "    X = np.apply_along_axis(lambda x : x[0], 1, series)\n",
    "    y_labels =  np.array(labels_batch)\n",
    "\n",
    "    return X, y_labels\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "\n",
    "# print(X[:])\n",
    "# print(y_labels[1:10])\n",
    "\n",
    "# Simulating a streaming\n",
    "data_stream = stream_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the 45000 data from the entire data set.\n",
    "for i in range(4):\n",
    "    print(\"range %i\" % i)\n",
    "    X_train, y_labels_train = generate_X_y_labels(data_stream, 1000)\n",
    "    if not len(X_train): break\n",
    "        \n",
    "    model_sgd = clf.partial_fit(X_train, y_labels_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hujol/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py:25: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 1\n",
      "range 2\n",
      "range 3\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Test on the last 5000 entries.\n",
    "X_test, y_labels_test = generate_X_y_labels(data_stream, 5000)\n",
    "\n",
    "print(X_test)\n",
    "if len(X_test):\n",
    "    print(\"\\nscore: %.3f\" % model_sgd.score(X_test, y_labels_test))\n",
    "else:\n",
    "    print('No data')\n",
    "\n",
    "# Train the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark V2.3.2 (Local)",
   "language": "python",
   "name": "pyspark-2.3.2-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
