{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using Latent Dirichlet Allocation (Clustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is using [Stanford IMDb Review dataset](http://ai.stanford.edu/~amaas/data/sentiment \"Stanford IMDb Large Movie Review Dataset\").\n",
    "One must download it, install it locally and set up the variable 'base_path' below to the FS path of the dataset.\n",
    "\n",
    "This notebook is about topic modeling using a technique called Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['README', 'aclImdb_100000.csv', 'aclImdb_100000_raw.parquet', 'aclImdb_10000_raw.parquet', 'aclImdb_1000_raw.parquet', 'aclImdb_100_raw.parquet', 'aclImdb_20000_raw.parquet', 'aclImdb_2000_raw.parquet', 'aclImdb_200_raw.parquet', 'aclImdb_210_raw.parquet', 'aclImdb_211_raw.parquet', 'aclImdb_250.csv', 'aclImdb_250_raw.parquet', 'aclImdb_251_raw.parquet', 'aclImdb_252_raw.parquet', 'aclImdb_300_raw.parquet', 'aclImdb_301_raw.parquet', 'aclImdb_50000_raw.parquet', 'imdb.vocab', 'imdbEr.txt', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# Set the base of the data path where folders test/neg, train/pos, etc, live.\n",
    "base_path = \"../../data/aclImdb\"\n",
    "\n",
    "# The folders where to look for the reviews.\n",
    "data_sets = ['test', 'train']\n",
    "sa_dir_names = ['neg', 'pos']\n",
    "\n",
    "# List the content of the data path for the sake of checking the data set folders.\n",
    "files = !ls {base_path}\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "LDA works on numbers and not on text. The data has to be converted into a feature vector representation for LDA to be able to compute metrics. The metrics will then serve to define clusters and group observations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Python system path to find our modules.\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import our modules.\n",
    "import file_loader as fl\n",
    "\n",
    "# Add the file to SparkContext for the executor to find it.\n",
    "sc.addPyFile('../src/file_loader.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data in a parquet file.\n",
    "file_parquet, _ = fl.load_data(base_path, 301, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m../../data/aclImdb/aclImdb_100000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_10000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_1000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_100_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_20000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_2000_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_200_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_210_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_211_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_250_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_251_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_252_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_300_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_301_raw.parquet\u001b[m\u001b[m\n",
      "\u001b[34m../../data/aclImdb/aclImdb_50000_raw.parquet\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -d {base_path}/*.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../../data/aclImdb/aclImdb_301_raw.parquet'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the parquet file into a data frame.\n",
    "df_pqt = spark.read.parquet(file_parquet)\n",
    "\n",
    "# As needed.\n",
    "# df_pqt = df_pqt.drop('words')\n",
    "\n",
    "# Showing some observations (entries).\n",
    "df_pqt.persist()\n",
    "df_pqt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/hujol/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['both',\n",
       " 'was',\n",
       " 'been',\n",
       " 'ourselves',\n",
       " 'doing',\n",
       " 'because',\n",
       " 'am',\n",
       " 'as',\n",
       " 'and',\n",
       " 'me']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Remove the stop words\n",
    "nltk.download('stopwords')\n",
    "stopwords_set = list(set(stopwords.words('english')))\n",
    "\n",
    "stopwords_set[:10]\n",
    "# stopwords_bc = spark.sparkContext.broadcast(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Remove all HTML tags.\n",
    "df_pqt = fl.transform_html_clean(df_pqt, 'textclean')\n",
    "\n",
    "# Tokenize and remove stop words.\n",
    "tokenizer = Tokenizer(inputCol=\"textclean\", outputCol=\"words_tknz\")\n",
    "remover = StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol=\"words_test\", \n",
    "                           stopWords=stopwords_set)\n",
    "pipeline = Pipeline(stages=[tokenizer, remover])\n",
    "\n",
    "# Fit the pipeline.\n",
    "model_p = pipeline.fit(df0)\n",
    "\n",
    "# Tranform the data frame.\n",
    "df_p = model_p.transform(df0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3471 ll back on the tried and true toilet humor of a teen sex comedy [i.e. \"American Pie\"], or warm the audience with the sentimentality of a romantic comedy [i.e. Julia Roberts' entire career]. It can only maintain a push to the end, and hope that the audience can appreciate the almost required irony of it's resolution.<br /><br />Written by husband/wi\n",
      "3377 ll back on the tried and true toilet humor of a teen sex comedy i.e. \"American Pie\", or warm the audience with the sentimentality of a romantic comedy i.e. Julia Roberts' entire career. It can only maintain a push to the end, and hope that the audience can appreciate the almost required irony of it's resolution.Written by husband/wife team Wally Wo\n",
      "327 ['sophisticated', 'sex', 'comedies', 'always', 'difficult', 'pull', 'off.', 'look', 'films', 'blake']\n"
     ]
    }
   ],
   "source": [
    "# Check the resulting transformation.\n",
    "len(df_p.head().words_test)\n",
    "a_sample = df_p.take(5)[4]\n",
    "print(len(a_sample['text']), a_sample['text'][250:600])\n",
    "print(len(a_sample['textclean']), a_sample['textclean'][250:600])\n",
    "print(len(a_sample['words_test']), a_sample['words_test'][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 22)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the df into train and test\n",
    "df_p_training, df_p_test = df_p.randomSplit([0.9, 0.1], seed=12345)\n",
    "\n",
    "df_p_training.count(), df_p_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a features vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_training = df_p_training.drop('featurestf')\n",
    "\n",
    "# Define the count vector so the IDF can compute the features vector.\n",
    "cv = CountVectorizer(inputCol=remover.getOutputCol(), outputCol=\"featurestf\", vocabSize=30000, minDF=1.0)\n",
    "idf = IDF(inputCol=cv.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Create the pipeline.\n",
    "pipeline = Pipeline(stages=[cv, idf])\n",
    "\n",
    "# Fit the pipeline.\n",
    "model_idf = pipeline.fit(df_p_training)\n",
    "\n",
    "# Transform the data frame.\n",
    "df_idf = model_idf.transform(df_p_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(10578, {0: 4.8218, 1: 2.9677, 2: 5.9311, 3: 3.4441, 4: 1.073, 5: 3.3351, 7: 4.0943, 8: 2.1714, 9: 3.2571, 10: 2.7639, 12: 2.5055, 15: 1.3649, 17: 1.5712, 19: 1.4534, 20: 1.4534, 22: 1.6138, 23: 1.4534, 25: 1.6582, 29: 1.6358, 32: 3.5577, 36: 1.6812, 37: 1.8048, 39: 5.2606, 40: 3.3165, 43: 2.1466, 44: 3.7741, 45: 1.8048, 46: 3.6097, 47: 1.7288, 48: 5.4945, 51: 1.8048, 57: 1.9767, 58: 3.8321, 59: 4.1502, 61: 1.9767, 62: 6.3306, 66: 2.1102, 67: 2.3514, 68: 2.0084, 70: 6.2254, 72: 6.4397, 74: 2.3979, 78: 2.1843, 85: 2.0751, 87: 6.6706, 88: 9.5916, 94: 2.2644, 95: 2.1843, 100: 4.6138, 102: 2.3979, 104: 2.2235, 106: 2.2644, 107: 2.3514, 109: 4.7958, 116: 4.6138, 117: 4.7958, 118: 4.7028, 121: 2.2644, 132: 2.4467, 139: 2.552, 141: 2.498, 144: 7.6561, 150: 2.6092, 159: 2.6092, 164: 2.6698, 171: 5.6067, 179: 2.552, 181: 2.552, 184: 5.4687, 192: 2.6698, 202: 5.6067, 203: 2.7344, 206: 5.3397, 209: 22.9979, 219: 2.8034, 247: 2.8775, 250: 2.7344, 256: 2.8034, 258: 5.7549, 271: 2.8034, 272: 3.0445, 287: 6.089, 290: 3.2452, 304: 3.0445, 310: 2.8775, 315: 2.9575, 334: 3.0445, 335: 9.4195, 338: 9.7356, 344: 3.363, 353: 3.0445, 354: 9.4195, 357: 3.833, 359: 10.0889, 363: 3.0445, 371: 3.2452, 374: 3.363, 411: 3.0445, 423: 6.2797, 424: 3.0445, 426: 3.0445, 430: 3.1398, 444: 3.2452, 451: 3.1398, 456: 3.1398, 481: 3.2452, 548: 3.2452, 555: 3.2452, 568: 3.2452, 570: 10.4895, 583: 6.993, 588: 3.4965, 603: 3.4965, 640: 3.6507, 641: 3.363, 651: 3.4965, 655: 3.363, 679: 3.363, 698: 3.4965, 706: 3.6507, 708: 3.4965, 710: 3.4965, 717: 3.4965, 725: 3.4965, 733: 3.4965, 745: 3.6507, 753: 7.3013, 775: 7.3013, 807: 3.4965, 808: 3.4965, 868: 3.6507, 870: 7.666, 876: 3.833, 882: 17.3752, 905: 3.6507, 911: 3.6507, 917: 12.1684, 929: 3.833, 942: 3.6507, 952: 3.833, 955: 3.6507, 959: 3.833, 992: 8.1122, 1012: 7.666, 1016: 3.833, 1035: 3.833, 1041: 3.833, 1047: 4.0561, 1048: 3.833, 1053: 3.833, 1078: 3.833, 1095: 3.833, 1122: 3.833, 1132: 3.833, 1140: 3.833, 1141: 3.833, 1158: 4.0561, 1161: 3.833, 1199: 8.1122, 1220: 3.833, 1244: 3.833, 1250: 3.833, 1263: 8.1122, 1290: 3.833, 1366: 4.0561, 1425: 4.0561, 1432: 4.0561, 1454: 4.0561, 1467: 4.3438, 1497: 4.0561, 1499: 4.0561, 1517: 4.0561, 1538: 4.0561, 1549: 4.0561, 1579: 4.0561, 1591: 4.0561, 1607: 4.0561, 1619: 8.6876, 1667: 4.0561, 1670: 4.0561, 1709: 4.0561, 1747: 8.6876, 1759: 4.0561, 1772: 4.0561, 1811: 4.3438, 1824: 4.0561, 1825: 4.0561, 1910: 4.3438, 1929: 4.0561, 1933: 4.0561, 1940: 4.0561, 1978: 4.3438, 2009: 4.3438, 2056: 4.3438, 2069: 4.3438, 2086: 4.3438, 2210: 4.3438, 2245: 4.3438, 2253: 4.3438, 2304: 4.3438, 2305: 9.4985, 2306: 4.3438, 2380: 4.3438, 2390: 4.3438, 2458: 4.3438, 2474: 4.3438, 2526: 4.3438, 2550: 4.3438, 2553: 9.4985, 2556: 4.3438, 2562: 4.3438, 2611: 9.4985, 2654: 4.3438, 2781: 9.4985, 2790: 4.3438, 2795: 4.3438, 2808: 4.3438, 2854: 4.3438, 2904: 4.3438, 2937: 4.3438, 3039: 4.3438, 3083: 4.3438, 3099: 4.3438, 3104: 4.3438, 3106: 4.3438, 3118: 4.3438, 3196: 4.3438, 3299: 4.3438, 3342: 4.3438, 3356: 4.3438, 3361: 4.3438, 3387: 4.7493, 3578: 4.7493, 3764: 4.7493, 3806: 4.7493, 3874: 4.7493, 3925: 4.7493, 4067: 4.7493, 4313: 4.7493, 4404: 4.7493, 4428: 4.7493, 4518: 4.7493, 4620: 4.7493, 4629: 4.7493, 4656: 4.7493, 4803: 4.7493, 4832: 4.7493, 4960: 4.7493, 4967: 4.7493, 5124: 4.7493, 5188: 4.7493, 5204: 4.7493, 5234: 4.7493, 5336: 4.7493, 5544: 4.7493, 5633: 4.7493, 5638: 4.7493, 5764: 4.7493, 5804: 4.7493, 5805: 4.7493, 5811: 4.7493, 5835: 4.7493, 5854: 4.7493, 5886: 4.7493, 5910: 4.7493, 6015: 4.7493, 6179: 4.7493, 6265: 4.7493, 6267: 4.7493, 6381: 4.7493, 6636: 4.7493, 6847: 4.7493, 6874: 4.7493, 7107: 4.7493, 7163: 4.7493, 7233: 4.7493, 7425: 4.7493, 7459: 4.7493, 7529: 4.7493, 7652: 4.7493, 7706: 4.7493, 7759: 4.7493, 7930: 4.7493, 8111: 4.7493, 8189: 4.7493, 8374: 4.7493, 8379: 4.7493, 8418: 4.7493, 8686: 4.7493, 8689: 4.7493, 8735: 4.7493, 8760: 4.7493, 8916: 4.7493, 8963: 4.7493, 8974: 4.7493, 9122: 4.7493, 9126: 4.7493, 9176: 4.7493, 9196: 4.7493, 9265: 4.7493, 9320: 4.7493, 9343: 4.7493, 9427: 4.7493, 9437: 4.7493, 9528: 4.7493, 9597: 4.7493, 9687: 4.7493, 9746: 4.7493, 9876: 4.7493, 9942: 4.7493, 10203: 4.7493, 10323: 4.7493, 10355: 4.7493, 10376: 4.7493, 10390: 4.7493})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result.\n",
    "a_sample = df_idf.take(1)[0]\n",
    "a_sample['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation Applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "lda = LDA(k=5, seed=1, optimizer=\"em\")\n",
    "model_lda = lda.fit(df_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10578"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the result.\n",
    "model_lda.vocabSize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>termIndices</th>\n",
       "      <th>termWeights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 1, 9, 74, 624, 4, 52, 281, 6, 19]</td>\n",
       "      <td>[0.0020609759941865036, 0.002036337672082326, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[60, 55, 1, 474, 307, 0, 14, 2, 88, 41]</td>\n",
       "      <td>[0.003516612765568113, 0.002261082291291362, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[6, 0, 453, 1, 2, 81, 3, 47, 209, 221]</td>\n",
       "      <td>[0.003052733224759423, 0.0026427669067573475, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[443, 14, 2, 25, 1, 15, 33, 21, 10, 0]</td>\n",
       "      <td>[0.00213380505133751, 0.002106536718676652, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[153, 6, 410, 5, 465, 1, 156, 2, 119, 0]</td>\n",
       "      <td>[0.0038359394566674686, 0.0024447223590275805,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic                               termIndices  \\\n",
       "0      0     [0, 1, 9, 74, 624, 4, 52, 281, 6, 19]   \n",
       "1      1   [60, 55, 1, 474, 307, 0, 14, 2, 88, 41]   \n",
       "2      2    [6, 0, 453, 1, 2, 81, 3, 47, 209, 221]   \n",
       "3      3    [443, 14, 2, 25, 1, 15, 33, 21, 10, 0]   \n",
       "4      4  [153, 6, 410, 5, 465, 1, 156, 2, 119, 0]   \n",
       "\n",
       "                                         termWeights  \n",
       "0  [0.0020609759941865036, 0.002036337672082326, ...  \n",
       "1  [0.003516612765568113, 0.002261082291291362, 0...  \n",
       "2  [0.003052733224759423, 0.0026427669067573475, ...  \n",
       "3  [0.00213380505133751, 0.002106536718676652, 0....  \n",
       "4  [0.0038359394566674686, 0.0024447223590275805,...  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.describeTopics().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = model_lda.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.82449917, 37.14671488, 50.37797794, 29.70896658, 34.68058176],\n",
       "       [36.38427382, 42.43309555, 36.06745967, 31.73441111, 38.86509604],\n",
       "       [24.40049702, 35.97351675, 35.84170214, 37.58443714, 35.65941904],\n",
       "       [22.11280188, 31.05716164, 31.38364112, 21.02460429, 30.1208124 ],\n",
       "       [33.12244131, 18.21111216, 24.59345038, 19.48178838, 29.05571131],\n",
       "       [19.89510702, 25.04741253, 24.73496457, 15.64804686, 43.62985596],\n",
       "       [30.68420526, 30.93673338, 58.19299715, 28.40444837, 50.59886487],\n",
       "       [26.12092055, 23.08888505, 21.64141218, 15.86207514, 27.92734258],\n",
       "       [26.70890323, 24.35316999, 20.86747695, 20.84783924, 25.56487893],\n",
       "       [33.69699164, 15.63287622, 13.26981784, 20.41148247, 25.55972021]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_matrix.toArray()[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv = cv.fit(df_p_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'film', 'like', 'one', 'good', 'really', '-', 'would', 'even', 'see', 'get', 'time', 'think', 'great', 'people', 'story', 'much', 'first', 'it.', 'also']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_cv.vocabulary[:20])\n",
    "'it' in stopwords_set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark V2.3.2 (Local)",
   "language": "python",
   "name": "pyspark-2.3.2-local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
